{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LDA Topic Modeling of CORD19 Data</h1>\n",
    "<h3>Morgan VandenBerg</h3>\n",
    "\n",
    "This notebook provides utilities to generate and explore topic models on the CORD19 dataset, found on Kaggle at:\n",
    "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/\n",
    "\n",
    "Here is a link to the `topic_models.csv` file that can be read in to avoid processing the first section of this notebook: https://smu.box.com/s/3t52kyb3jeeftd8q4w3zd9bl59ba6x9b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is a generic topic model builder class adapted from SMU's CS7391 Special Topics (Natural Language Processing) second homework project. Note that the `NUM_CORES` parameter should be left at 1 for the CORD19 data if processing individual paragraphs, as the multithreading overhead is too high to increase efficiency on these small documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Allows me to speed up tagging without using pos_tag_sents\n",
    "# as per https://stackoverflow.com/questions/33676526/pos-tagger-is-incredibly-slow\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import nltk\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "NUM_CORES = 1\n",
    "\n",
    "class LDAModelBuilder:\n",
    "    _stemLemmaTool = None\n",
    "    _stemDictionary = {}\n",
    "    _tagger = PerceptronTagger()\n",
    "\n",
    "    def __init__(self, numTopics, vectorModel, alpha, useToken, usePOS, useStemLemma, stopwordsFile, outputFile, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self._numTopics = numTopics\n",
    "        self._vectorModel = vectorModel\n",
    "        self._alpha = alpha\n",
    "        self._useToken = useToken\n",
    "        self._usePOS = usePOS\n",
    "        self._useStemLemma = useStemLemma\n",
    "        self.__initStopWords(stopwordsFile)\n",
    "        self._outputFile = outputFile\n",
    "\n",
    "    def __initStopWords(self, stopwordsFile):\n",
    "        if stopwordsFile == None:\n",
    "            if self.verbose:\n",
    "                print(\"\\tSelected 'none' for stopwords.\")\n",
    "            self._stopwords = set()\n",
    "        elif stopwordsFile == 'nltk':\n",
    "            if self.verbose:\n",
    "                print(\"\\tSelected NLTK stopwords.\")\n",
    "            self._stopwords = set(nltk_stopwords.words('english'))\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"\\tReading custom stopwords from file.\")\n",
    "            self._stopwords = set(line.strip().lower()\n",
    "                                  for line in open(stopwordsFile))\n",
    "\n",
    "    def getStopwordSet(self):\n",
    "        return self._stopwords\n",
    "\n",
    "    def __stemOrLemmatizeDocument(self, document):\n",
    "        # Stem or lemmatize document given program config\n",
    "        if self._useStemLemma == 'N':\n",
    "            return document\n",
    "        elif self._useStemLemma == 'B':\n",
    "            return self.__stemDocument(document)\n",
    "        elif self._useStemLemma == 'L':\n",
    "            return self.__lemmatizeDoc(document)\n",
    "        else:\n",
    "            print(\"Unsupported stem/lemmatization setting given in config file.\")\n",
    "\n",
    "    def __stemDocument(self, documentTokens):\n",
    "        # Stem documents using PorterStemmer\n",
    "        toStem = []\n",
    "        # process document to remove parts of speech as specified, since lemmatization function will do this automatically\n",
    "        partsSpeech = self._tagger.tag(documentTokens)\n",
    "        for word, tag in partsSpeech:\n",
    "            wntag = self.__getWordnetTag(tag)\n",
    "            if self.__keepPartOfSpeech(wntag):\n",
    "                toStem.append(word)\n",
    "\n",
    "        # Pass items through stemmer, memoizing / referencing dictionary for performance\n",
    "        toReturn = []\n",
    "        if self._stemLemmaTool is None:\n",
    "            self._stemLemmaTool = PorterStemmer()\n",
    "        for word in toStem:\n",
    "            if word not in self._stemDictionary:\n",
    "                self._stemDictionary[word] = self._stemLemmaTool.stem(word)\n",
    "            toReturn.append(self._stemDictionary[word])\n",
    "\n",
    "        return toReturn\n",
    "\n",
    "    def __lemmatizeDoc(self, documentTokens):\n",
    "        # Lemmatize the document tokens using NLTK pos_tag\n",
    "        toReturn = []\n",
    "        if self._stemLemmaTool is None:\n",
    "            self._stemLemmaTool = WordNetLemmatizer()\n",
    "        partsSpeech = self._tagger.tag(documentTokens)\n",
    "\n",
    "        for word, tag in partsSpeech:\n",
    "            wntag = self.__getWordnetTag(tag)\n",
    "            lemma = None\n",
    "            if self.__keepPartOfSpeech(wntag):\n",
    "                if wntag is None:\n",
    "                    lemma = self._stemLemmaTool.lemmatize(word)\n",
    "                else:\n",
    "                    lemma = self._stemLemmaTool.lemmatize(word, pos=wntag)\n",
    "                toReturn.append(lemma)\n",
    "        return toReturn\n",
    "\n",
    "    def __keepPartOfSpeech(self, pos):\n",
    "        # Determine if the word should be kept given its part of speech and program config\n",
    "        if self._usePOS == 'A':\n",
    "            return True\n",
    "        elif self._usePOS == 'F':\n",
    "            return (pos == wordnet.NOUN or pos == wordnet.VERB or pos == wordnet.ADJ or pos == wordnet.ADV)\n",
    "        elif self._usePOS == 'N':\n",
    "            return (pos == wordnet.NOUN or pos == wordnet.ADJ)\n",
    "        elif self._usePOS == 'n':\n",
    "            return (pos == wordnet.NOUN)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __getWordnetTag(self, tag):\n",
    "        # Convert to WordNet tags (from Penn)\n",
    "        # Source for this method: https://stackoverflow.com/a/15590384\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __isStopWord(self, word):\n",
    "        if word.lower() in self._stopwords:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __processWordToKeep(self, word):\n",
    "        if self._useToken == 'A':\n",
    "            # Keep all words except single character non-alphanumeric characters\n",
    "            if len(word) == 1 and re.search(r'\\W', word):\n",
    "                # Case for single-character nonalphanumeric\n",
    "                return None\n",
    "            else:\n",
    "                return word\n",
    "\n",
    "        elif self._useToken == 'a':\n",
    "            # Keep all words except single character non-alphanumeric characters,\n",
    "            # remove symbols if token is a mixture of alphanumeric and symbols\n",
    "            if len(word) == 1 and re.search(r'\\W', word):\n",
    "                # Case for single-character nonalphanumeric\n",
    "                return None\n",
    "            else:\n",
    "                return re.sub(r'\\W', '', word)\n",
    "\n",
    "        elif self._useToken == 'N':\n",
    "            # Keep only alphanumeric tokens\n",
    "            if re.search(r'\\W', word):\n",
    "                # Case for non-alphanumeric\n",
    "                return None\n",
    "            else:\n",
    "                # Valid case\n",
    "                return word\n",
    "\n",
    "        elif self._useToken == 'n':\n",
    "            # Keep only alphanumeric tokens, removing tokens that are only numbers\n",
    "            if re.search(r'\\W', word):\n",
    "                # Case for non-alphanumeric\n",
    "                return None\n",
    "            if not re.search(r'[a-zA-Z]', word):\n",
    "                # Case for only numbers\n",
    "                return None\n",
    "            else:\n",
    "                # Valid case\n",
    "                return word\n",
    "\n",
    "    def preProcessDocument(self, doc):\n",
    "        return self.__preProcessDocument(word_tokenize(doc))\n",
    "\n",
    "    def getBagOfWords(self, tokens):\n",
    "        return self.id2word_.doc2bow(tokens)\n",
    "\n",
    "    def __preProcessDocument(self, tokens):\n",
    "        # Perform stemming or lemmatization\n",
    "        firstPass = []\n",
    "        for word in tokens:\n",
    "            if len(word) < 3:\n",
    "                continue\n",
    "            word=word.lower()\n",
    "            keepWord = self.__processWordToKeep(word)\n",
    "            if keepWord is not None and not self.__isStopWord(keepWord):\n",
    "                firstPass.append(keepWord)\n",
    "        firstPass = self.__stemOrLemmatizeDocument(firstPass)\n",
    "        # Now we remove stop words again\n",
    "        toReturn = []\n",
    "        for word in firstPass:\n",
    "            if not self.__isStopWord(word):\n",
    "                toReturn.append(word)\n",
    "\n",
    "        return toReturn\n",
    "\n",
    "    def __buildGensimCorpus(self, documents):\n",
    "        if self.verbose:\n",
    "            print(\"\\tBuilding GenSim corpus...\")\n",
    "        # Build a GenSim corpus given documents\n",
    "        processedDocuments = []\n",
    "        for doc in documents:\n",
    "            processedDocuments.append(self.preProcessDocument(doc))\n",
    "\n",
    "        wordIDs = corpora.Dictionary(processedDocuments)\n",
    "        corpus = [wordIDs.doc2bow(text) for text in processedDocuments]\n",
    "\n",
    "        if self._vectorModel == 'B':\n",
    "            # Use binary model\n",
    "            for document in corpus:\n",
    "                document[:] = [(id, 1 if freq > 0 else 0)\n",
    "                               for (id, freq) in document]\n",
    "\n",
    "        elif self._vectorModel == 'T':\n",
    "            # Use TFIDF model\n",
    "            tfidf = gensim.models.TfidfModel(corpus)\n",
    "            corpus = tfidf[corpus]\n",
    "\n",
    "        # Else 't' use TF model (no adjustment)\n",
    "        elif not self._vectorModel == 't':\n",
    "            print(\"Unsupported vector model passed in config file.\")\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\tBuilt GenSim corpus.\")\n",
    "        return (corpus, wordIDs)\n",
    "\n",
    "    def __buildLDAModel(self, corpus, id2word):\n",
    "        return gensim.models.ldamulticore.LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            num_topics=self._numTopics,\n",
    "            alpha=self._alpha,\n",
    "            workers=NUM_CORES\n",
    "        )\n",
    "\n",
    "    def trainLDA(self, documents):\n",
    "        trainSuccess = False\n",
    "        if self.verbose:\n",
    "            print(\"\\tBuilding GenSim LDA topic model...\")\n",
    "        # Build / train LDA model via GenSim\n",
    "        corpus, wordIDs = self.__buildGensimCorpus(documents)\n",
    "        if len(wordIDs) > 0:\n",
    "            self.LDAmodel_ = self.__buildLDAModel(corpus, wordIDs)\n",
    "            self.id2word_ = wordIDs\n",
    "            self._corpus_ = corpus\n",
    "            trainSuccess = True\n",
    "            if self.verbose:\n",
    "                print(\"\\tBuilt GenSim LDA topic model.\")\n",
    "        return trainSuccess\n",
    "    \n",
    "    def saveModel_(self):\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaving LDA topic model...\")\n",
    "        # Output the model, note I use SKLearn convention with pre/post-underscore to denote pre/post train functions\n",
    "        self.LDAmodel_.save(self._outputFile + '.model')\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaved LDA topic model.\")\n",
    "\n",
    "    def loadModel(self, fromFile):\n",
    "        self.LDAmodel_ = gensim.models.ldamulticore.LdaMulticore.load(fromFile)\n",
    "            \n",
    "    def getTopic(self, topicID, n=10):\n",
    "        topic = self.LDAmodel_.get_topic_terms(topicid=topicID, topn=n)\n",
    "        # Transform word IDs back to the original word\n",
    "        topic[:] = [(self.id2word_[id], prob) for (id, prob) in topic]\n",
    "        return topic\n",
    "\n",
    "    def saveTopics_(self, n=10):\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaving LDA topics...\")\n",
    "        for topicID in range(0, self._numTopics):\n",
    "            topic = self.getTopic(topicID=topicID, n=n)\n",
    "            # Write topic output\n",
    "            with open(self._outputFile + '_' + str(topicID) + '.topic', 'w') as writer:\n",
    "                for (word, prob) in topic:\n",
    "                    writer.write(word)\n",
    "                    writer.write(' ')\n",
    "                    writer.write(str(prob))\n",
    "                    writer.write('\\n')\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaved LDA topics.\")\n",
    "\n",
    "    def generateAndSaveDocTopics_(self, fileNames):\n",
    "        if self.verbose:\n",
    "            print(\"\\tGenerating and saving document topics...\")\n",
    "        # Pass file names in as vector corresponding to original corpus documents\n",
    "        with open(self._outputFile + '.dt', 'w') as writer:\n",
    "            # Iterate over documents / filenames simultaneously\n",
    "            for document, fileName in zip(self._corpus_, fileNames):\n",
    "                writer.write(fileName)\n",
    "                writer.write(' ')\n",
    "                # Get topics for document\n",
    "                docTopics = self.LDAmodel_.get_document_topics(\n",
    "                    document, minimum_probability=0)\n",
    "                # Write document topics to file\n",
    "                for (topicID, prob) in docTopics:\n",
    "                    writer.write(str(prob))\n",
    "                    writer.write(' ')\n",
    "                writer.write('\\n')\n",
    "        print(\"\\tGenerated and saved document topics.\")\n",
    "\n",
    "\n",
    "def getJaccard(s, t):\n",
    "    # Given two sets of words\n",
    "    # Calculate the Jaccard coefficient | S ⋂ T | / | S ⋃ T |\n",
    "    numer = len(s.intersection(t))\n",
    "    denom = len(s.union(t))\n",
    "    return numer / denom if denom > 0 else 0\n",
    "\n",
    "\n",
    "def getTopicSim(t1, t2):\n",
    "    # Given two topics, get their similarity as Jaccard of T1(k) and T2(k)\n",
    "    # Format note: t1, t2 should be sets of tuple representing (topic, topic_prob)\n",
    "    t1_words = set()\n",
    "    t2_words = set()\n",
    "    for ((t1_word, _), (t2_word, _)) in zip(t1, t2):\n",
    "        t1_words.add(t1_word)\n",
    "        t2_words.add(t2_word)\n",
    "    sim = getJaccard(t1_words, t2_words)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def getTopicSetSim(tprime, uprime):\n",
    "    selectedUvals = set()  # Used to see if we get a perfect match\n",
    "    simSum = 0\n",
    "    for t in tprime:\n",
    "        bestTopic = None\n",
    "        bestSim = None\n",
    "        counter = 0\n",
    "        bestTopicIndex = 0\n",
    "        for u in uprime:\n",
    "            if bestTopic is None:\n",
    "                # First index: assign base best topic, sim, index\n",
    "                bestTopic = u\n",
    "                bestTopicIndex = 0\n",
    "                bestSim = getTopicSim(t, u)\n",
    "                continue\n",
    "            # Get similarity for current topic\n",
    "            sim = getTopicSim(t, u)\n",
    "            if sim > bestSim:\n",
    "                bestSim = sim\n",
    "                bestTopic = u\n",
    "                bestTopicIndex = counter\n",
    "            counter += 1\n",
    "        selectedUvals.add(bestTopicIndex)  # Selected U at index (counter)\n",
    "        simSum += bestSim\n",
    "    if len(selectedUvals) == len(tprime):\n",
    "        # Perfect match was found\n",
    "        return (None, simSum)\n",
    "    else:\n",
    "        # Did not find a perfect match\n",
    "        return (\n",
    "            # First term: number of selected topics / number of topics in T\n",
    "            len(selectedUvals) / len(tprime),\n",
    "            simSum\n",
    "        )\n",
    "\n",
    "\n",
    "def getWordnetTag(tag):\n",
    "    # Convert to WordNet tags (from Penn)\n",
    "    # Source for this method: https://stackoverflow.com/a/15590384\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord19Path = './../scratch/CORD19Data/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xqhn0vbp</td>\n",
       "      <td>1e1286db212100993d03cc22374b624f7caee956</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Airborne rhinovirus detection and effect of ul...</td>\n",
       "      <td>10.1186/1471-2458-3-5</td>\n",
       "      <td>PMC140314</td>\n",
       "      <td>12525263.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: Rhinovirus, the most common cause ...</td>\n",
       "      <td>2003-01-13</td>\n",
       "      <td>Myatt, Theodore A; Johnston, Sebastian L; Rudn...</td>\n",
       "      <td>BMC Public Health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gi6uaa83</td>\n",
       "      <td>8ae137c8da1607b3a8e4c946c07ca8bda67f88ac</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Discovering human history from stomach bacteria</td>\n",
       "      <td>10.1186/gb-2003-4-5-213</td>\n",
       "      <td>PMC156578</td>\n",
       "      <td>12734001.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Recent analyses of human pathogens have reveal...</td>\n",
       "      <td>2003-04-28</td>\n",
       "      <td>Disotell, Todd R</td>\n",
       "      <td>Genome Biol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>le0ogx1s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC</td>\n",
       "      <td>A new recruit for the army of the men of death</td>\n",
       "      <td>10.1186/gb-2003-4-7-113</td>\n",
       "      <td>PMC193621</td>\n",
       "      <td>12844350.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>The army of the men of death, in John Bunyan's...</td>\n",
       "      <td>2003-06-27</td>\n",
       "      <td>Petsko, Gregory A</td>\n",
       "      <td>Genome Biol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fy4w7xz8</td>\n",
       "      <td>0104f6ceccf92ae8567a0102f89cbb976969a774</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Association of HLA class I with severe acute r...</td>\n",
       "      <td>10.1186/1471-2350-4-9</td>\n",
       "      <td>PMC212558</td>\n",
       "      <td>12969506.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: The human leukocyte antigen (HLA) ...</td>\n",
       "      <td>2003-09-12</td>\n",
       "      <td>Lin, Marie; Tseng, Hsiang-Kuang; Trejaut, Jean...</td>\n",
       "      <td>BMC Med Genet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0qaoam29</td>\n",
       "      <td>5b68a553a7cbbea13472721cd1ad617d42b40c26</td>\n",
       "      <td>PMC</td>\n",
       "      <td>A double epidemic model for the SARS propagation</td>\n",
       "      <td>10.1186/1471-2334-3-19</td>\n",
       "      <td>PMC222908</td>\n",
       "      <td>12964944.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: An epidemic of a Severe Acute Resp...</td>\n",
       "      <td>2003-09-10</td>\n",
       "      <td>Ng, Tuen Wai; Turinici, Gabriel; Danchin, Antoine</td>\n",
       "      <td>BMC Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  xqhn0vbp  1e1286db212100993d03cc22374b624f7caee956      PMC   \n",
       "1  gi6uaa83  8ae137c8da1607b3a8e4c946c07ca8bda67f88ac      PMC   \n",
       "2  le0ogx1s                                       NaN      PMC   \n",
       "3  fy4w7xz8  0104f6ceccf92ae8567a0102f89cbb976969a774      PMC   \n",
       "4  0qaoam29  5b68a553a7cbbea13472721cd1ad617d42b40c26      PMC   \n",
       "\n",
       "                                               title                      doi  \\\n",
       "0  Airborne rhinovirus detection and effect of ul...    10.1186/1471-2458-3-5   \n",
       "1    Discovering human history from stomach bacteria  10.1186/gb-2003-4-5-213   \n",
       "2     A new recruit for the army of the men of death  10.1186/gb-2003-4-7-113   \n",
       "3  Association of HLA class I with severe acute r...    10.1186/1471-2350-4-9   \n",
       "4   A double epidemic model for the SARS propagation   10.1186/1471-2334-3-19   \n",
       "\n",
       "       pmcid   pubmed_id license  \\\n",
       "0  PMC140314  12525263.0   no-cc   \n",
       "1  PMC156578  12734001.0   no-cc   \n",
       "2  PMC193621  12844350.0   no-cc   \n",
       "3  PMC212558  12969506.0   no-cc   \n",
       "4  PMC222908  12964944.0   no-cc   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  BACKGROUND: Rhinovirus, the most common cause ...   2003-01-13   \n",
       "1  Recent analyses of human pathogens have reveal...   2003-04-28   \n",
       "2  The army of the men of death, in John Bunyan's...   2003-06-27   \n",
       "3  BACKGROUND: The human leukocyte antigen (HLA) ...   2003-09-12   \n",
       "4  BACKGROUND: An epidemic of a Severe Acute Resp...   2003-09-10   \n",
       "\n",
       "                                             authors            journal  \\\n",
       "0  Myatt, Theodore A; Johnston, Sebastian L; Rudn...  BMC Public Health   \n",
       "1                                   Disotell, Todd R        Genome Biol   \n",
       "2                                  Petsko, Gregory A        Genome Biol   \n",
       "3  Lin, Marie; Tseng, Hsiang-Kuang; Trejaut, Jean...      BMC Med Genet   \n",
       "4  Ng, Tuen Wai; Turinici, Gabriel; Danchin, Antoine     BMC Infect Dis   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN           True   \n",
       "1                          NaN            NaN           True   \n",
       "2                          NaN            NaN          False   \n",
       "3                          NaN            NaN           True   \n",
       "4                          NaN            NaN           True   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0               True  custom_license   \n",
       "1               True  custom_license   \n",
       "2               True  custom_license   \n",
       "3               True  custom_license   \n",
       "4               True  custom_license   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...  \n",
       "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_path = cord19Path + 'metadata.csv'\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code reads in the CORD19 metadata.\n",
    "\n",
    "The below code will read in the full dataset. Note that this takes a significant amount of time and consumes a large amount of memory. Moreover, it is NOT necessary to run other items in this notebook--only to generate the original topics, which takes ~30 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data loading code adapted from \n",
    "# https://github.com/elilaird/COVID-19-Open-Research-Dataset-Challenge/blob/master/CORD-19-Topic-Modeling.ipynb\n",
    "\n",
    "'''\n",
    "    @Desc    : Reads in json article and converts into Pandas Dataframe\n",
    "    @Params  : filepath (str)\n",
    "    @Returns : Pandas Dataframe \n",
    "'''\n",
    "def JsonToDataFrame(filepath):\n",
    "        \n",
    "    #read json into dict\n",
    "    with open(filepath) as json_data:\n",
    "        data = json.load(json_data)\n",
    "        \n",
    "        paper_id = data['paper_id']\n",
    "        abstract = '\\n'.join([section['text'] for section in data['abstract']])\n",
    "\n",
    "        \n",
    "\n",
    "        final_data = {\n",
    "            'paper_id'  : [data['paper_id']],\n",
    "            'section'   : ['abstract'],\n",
    "            'text'  : ['\\n'.join([section['text'] for section in data['abstract']])]                                       \n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(final_data)\n",
    "        for section in data['body_text']:\n",
    "            df = df.append({\n",
    "                'paper_id' : data['paper_id'],\n",
    "                'section'  : section['section'],\n",
    "                'text'     : section['text']\n",
    "            }, ignore_index=True)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "        \n",
    "biorxiv_medrxiv    = cord19Path + 'biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/'\n",
    "comm_use_subset    = cord19Path + 'comm_use_subset/comm_use_subset/pdf_json/'\n",
    "noncomm_use_subset = cord19Path + 'noncomm_use_subset/noncomm_use_subset/pdf_json/'\n",
    "\n",
    "biorxiv_medrxiv_files       = [biorxiv_medrxiv + pos_json for pos_json in os.listdir(biorxiv_medrxiv) if pos_json.endswith('.json')]\n",
    "comm_use_subset_files       = [comm_use_subset + pos_json for pos_json in os.listdir(comm_use_subset) if pos_json.endswith('.json')]\n",
    "noncomm_use_subset_files    = [noncomm_use_subset + pos_json for pos_json in os.listdir(noncomm_use_subset) if pos_json.endswith('.json')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Corpus loading code from https://github.com/elilaird/COVID-19-Open-Research-Dataset-Challenge/blob/clay_bert/CORD-19_BERT_clustering.ipynb\n",
    "# adapted with multiprocessing by Clay Harper\n",
    "\n",
    "import concurrent\n",
    "\n",
    "\n",
    "#initialize dfs\n",
    "biomed_df      = pd.DataFrame()\n",
    "comm_use_df    = pd.DataFrame()\n",
    "noncomm_use_df = pd.DataFrame()\n",
    "\n",
    "count = 0\n",
    "\n",
    "def to_df(file):\n",
    "    try:\n",
    "        return JsonToDataFrame(file)\n",
    "    except:\n",
    "        global count \n",
    "        count += 1\n",
    "        return None\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for _, df in zip(biorxiv_medrxiv_files, executor.map(to_df, biorxiv_medrxiv_files)):\n",
    "            if df is not None:\n",
    "                biomed_df = biomed_df.append(df, ignore_index=True)\n",
    "            \n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for _, df in zip(comm_use_subset_files, executor.map(to_df, comm_use_subset_files)):\n",
    "            if df is not None:\n",
    "                comm_use_df = comm_use_df.append(df, ignore_index=True)\n",
    "            \n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for _, df in zip(noncomm_use_subset_files, executor.map(to_df, noncomm_use_subset_files)):\n",
    "            if df is not None:\n",
    "                noncomm_use_df = noncomm_use_df.append(df, ignore_index=True)\n",
    "            \n",
    "print('Count of files with issues: {}'.format(count))\n",
    "full_corpus = pd.concat([biomed_df, comm_use_df, noncomm_use_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "#remove punctuation\n",
    "full_corpus['text'] = full_corpus['text'].map(lambda x: x.translate(punct_table))\n",
    "\n",
    "#convert to lowercase\n",
    "full_corpus['text'] = full_corpus['text'].map(lambda x: x.lower())\n",
    "\n",
    "full_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_corpus)  # number of documents is almost 500k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Topic Models Exploration</h1>\n",
    "\n",
    "I begin by creating a topic model for the 'prompt'; the thing that we want to know more about. Note that using a moderate-sized document here will help with the stability of the model. One should not run topic models for individual sentences in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seek to find groups of topics that match this prompt\n",
    "\n",
    "promptText = '''\n",
    "What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface?\n",
    "Specifically, we want to know what the literature reports about:\n",
    "Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\n",
    "Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\n",
    "Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.\n",
    "Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\n",
    "Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\n",
    "Experimental infections to test host range for this pathogen.\n",
    "Animal host(s) and any evidence of continued spill-over to humans\n",
    "Socioeconomic and behavioral risk factors for this spill-over\n",
    "Sustainable risk reduction strategies\n",
    "'''\n",
    "\n",
    "promptDocs = [promptText]\n",
    "\n",
    "baseModelOutputPath = '../scratch/CORD19_Topic_Models/'\n",
    "\n",
    "ldaParams = dict(\n",
    "    numTopics = 15,           # 15 topics\n",
    "    vectorModel = 't',        # term freq detection model\n",
    "    alpha = 5,                # higher alpha for sharp topic detection\n",
    "    useToken = 'n',           # strictest token filtering \n",
    "    usePOS = 'N',             # nouns or adv \n",
    "    useStemLemma = 'L',       # lemmatization\n",
    "    stopwordsFile = 'nltk',   # LDA stopwords\n",
    "    outputFile = baseModelOutputPath + 'prompt_base_model',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "topicsToMatch = LDAModelBuilder(\n",
    "    **ldaParams\n",
    ")\n",
    "\n",
    "topicsToMatch.trainLDA(promptDocs)\n",
    "topicsToMatch.saveTopics_()\n",
    "\n",
    "k = 15 # number of words per topic\n",
    "promptTopicSet = []\n",
    "for i in range(0, ldaParams['numTopics']):\n",
    "    topic = topicsToMatch.getTopic(topicID=i, n=k)\n",
    "    promptTopicSet.append(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generating CORD19 Topic Models</h1>\n",
    "\n",
    "The two below cells are used to build the full dataset of topic models. Please note that these cells take an incredibly long time to run--most users should instead skip this section and use the `topic_models.csv` file that I make openly available through SMU's Box service.\n",
    "\n",
    "The first cell is a single-threaded approach to generating models sequentially, the bottom cell will run a specified number of processes and split the work between them. \n",
    "\n",
    "NOTE: the workflow here requires that the generated topics are written to disk. There will be close to two million total files here; consider this before running the below code. I performed all work with these on SMU's supercomputer, ManeFrame II (M2). Once the models are generated, you MUST use the `TopicModelCSVGenerator.java` or `TopicModelCSVGeneratorThreaded.java` programs to convert the individual model files into a single CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Now get topics for each document\n",
    "ldaModels = dict()\n",
    "\n",
    "numEmpty=0\n",
    "\n",
    "minDocLength = 10\n",
    "\n",
    "for i in range(len(full_corpus)):\n",
    "    docText = [full_corpus['text'].iloc[i]]\n",
    "    ldaParams['outputFile'] = baseModelOutputPath + '_model_doc_' + str(i)\n",
    "    ldaModels[i] = LDAModelBuilder(**ldaParams)\n",
    "    if ldaModels[i].trainLDA(docText):\n",
    "        # Save if train successful\n",
    "        ldaModels[i].saveTopics_()\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        \n",
    "print(\"Processed\", i, \"documents.\")\n",
    "print(\"Found\", numEmpty,\"empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multiprocessing for above code\n",
    "from multiprocessing import Pool, Lock, Process\n",
    "from multiprocessing.sharedctypes import Array\n",
    "\n",
    "\n",
    "ldaModels = dict()\n",
    "\n",
    "numEmpty=0\n",
    "\n",
    "minDocLength = 10\n",
    "\n",
    "\n",
    "def processLDA(lowerIndex, upperIndex, documents):\n",
    "    for (doc, i) in zip(documents, range(lowerIndex, upperIndex)):\n",
    "        docText = [doc]\n",
    "        ldaParams['outputFile'] = baseModelOutputPath + '_model_doc_' + str(i)\n",
    "        model = LDAModelBuilder(**ldaParams)\n",
    "        if model.trainLDA(docText):\n",
    "            model.saveTopics_()\n",
    "\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    numCores = 36\n",
    "    totalDocs = len(full_corpus)\n",
    "\n",
    "    processes = []\n",
    "    splits = np.array_split(full_corpus, numCores)\n",
    "    \n",
    "    lower = 0\n",
    "    \n",
    "    for i in range(numCores):\n",
    "        # spawn process with docs\n",
    "        docs = splits[i].text.tolist()\n",
    "        p = Process(target=processLDA, args=(lower, lower+len(docs), docs))\n",
    "        p.start()\n",
    "        print(\"Started process\", i)\n",
    "        processes.append(p)\n",
    "\n",
    "        lower += len(docs)\n",
    "\n",
    "        \n",
    "        \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        print(\"Joined process.\")\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"Processed\", i, \"documents.\")\n",
    "    print(\"Found\", numEmpty,\"empty.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell provides a Pythononic way to read in the models. However, on most systems (including SMU's supercomputer), it is too inefficient to run. One should instead use the provided Java code if the individual topics must be read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in calculated topic models\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "import numpy\n",
    "\n",
    "ldaDocumentTopics = numpy.empty(500000, dtype=object)\n",
    "print(\"alloc array\")\n",
    "\n",
    "numFailed = 0\n",
    "\n",
    "numTopics = 15\n",
    "\n",
    "for i in range(500000):\n",
    "    readFile = baseModelOutputPath + '_model_doc_' + str(i)\n",
    "    \n",
    "    ldaDocumentTopics[i] = list()\n",
    "    \n",
    "    for t in range(numTopics):\n",
    "        topics = []\n",
    "        #if path.exists(readFile + '_' + str(t) + '.topic'):\n",
    "        try:\n",
    "            with open(readFile + '_' + str(t) + '.topic', 'r') as topicFile:\n",
    "                for topicLine in topicFile:\n",
    "                    items = topicLine.split()\n",
    "                    topicString = items[0]\n",
    "                    topicWeight = float(items[1])\n",
    "                    topics.append((topicString, topicWeight))\n",
    "        except OSError:\n",
    "            pass\n",
    "        ldaDocumentTopics[i].append(topics)      \n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"read in \", i, \"models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Exploring Topic Models</h1>\n",
    "\n",
    "This is the section that most users should skip to! Be sure to download the `topic_models.csv` file and place it in the appropriate directory before using the below code.\n",
    "\n",
    "Each line of that CSV corresponds to a word and weight from an individual topic. That belongs to one of 15 topics for any individual document. The `doc_id` field is ordered identically to the `full_corpus` object read in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>expert</td>\n",
       "      <td>0.048942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dynamic</td>\n",
       "      <td>0.046465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>result</td>\n",
       "      <td>0.045898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>license</td>\n",
       "      <td>0.044178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>current</td>\n",
       "      <td>0.043589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  topic_id     word    weight\n",
       "0       0         0   expert  0.048942\n",
       "1       0         0  dynamic  0.046465\n",
       "2       0         0   result  0.045898\n",
       "3       0         0  license  0.044178\n",
       "4       0         0  current  0.043589"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read saved topic models from disk\n",
    "import pandas as pd\n",
    "models = pd.read_csv('topic_models_mthread.csv')\n",
    "models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.696855e+07</td>\n",
       "      <td>6.696855e+07</td>\n",
       "      <td>6.696855e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.248791e+05</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>4.915377e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.297174e+05</td>\n",
       "      <td>4.320494e+00</td>\n",
       "      <td>4.923616e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.126180e+05</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>2.631349e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.248230e+05</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>3.767874e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.371080e+05</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>5.726257e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.498830e+05</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             doc_id      topic_id        weight\n",
       "count  6.696855e+07  6.696855e+07  6.696855e+07\n",
       "mean   2.248791e+05  7.000000e+00  4.915377e-02\n",
       "std    1.297174e+05  4.320494e+00  4.923616e-02\n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00\n",
       "25%    1.126180e+05  3.000000e+00  2.631349e-02\n",
       "50%    2.248230e+05  7.000000e+00  3.767874e-02\n",
       "75%    3.371080e+05  1.100000e+01  5.726257e-02\n",
       "max    4.498830e+05  1.400000e+01  1.000000e+00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Topic Similarity Matching</h2>\n",
    "\n",
    "I begin by finding paragraphs which match the prompt topics. The first approach below uses a simple Jaccard-coefficient based similarity measure. I will also explore techniques based on word-embeddings in later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "simScores = np.zeros((len(pd.unique(models.doc_id)), 1))\n",
    "\n",
    "# getTopicsForDocument will deterministically give the topics for a certain doc. But it is very, very slow. (10its/second)\n",
    "def getTopicsForDocument(docID):\n",
    "    topics = []\n",
    "    docTopics = models[ models.doc_id == docID ]\n",
    "    \n",
    "    for topicID in range(0, ldaParams['numTopics']):\n",
    "        topicItems = []\n",
    "        docTopic = docTopics[ docTopics.topic_id == topicID ]\n",
    "        for (topic, weight) in zip(docTopic.word.tolist(), docTopic.weight.tolist()):\n",
    "            topicItems.append( (topic, weight) )\n",
    "        topics.append(topicItems)\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# I can instead get topics very quickly if I index by rows\n",
    "def getTopicsFromRows(lower, upper, expectedID = None):\n",
    "    docTopics = models.iloc[lower:upper, :]\n",
    "    if expectedID is not None:\n",
    "        expectedLen = len(docTopics)\n",
    "        docTopics = docTopics[ docTopics.doc_id == expectedID ]\n",
    "        if len(docTopics) != expectedLen:\n",
    "            print(\"WARNING: at expected doc ID\",expectedID,\n",
    "                  \", number of items not matching expected ID:\",len(docTopics)-expectedLen)\n",
    "            \n",
    "    topics = []\n",
    "    for topicID in range(0, ldaParams['numTopics']):\n",
    "        topicItems = []\n",
    "        docTopic = docTopics[ docTopics.topic_id == topicID ]\n",
    "        for (topic, weight) in zip(docTopic.word.tolist(), docTopic.weight.tolist()):\n",
    "            topicItems.append( (topic, weight) )\n",
    "        topics.append(topicItems)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model (this may take some time)...\n",
      "Word2Vec model loaded.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: you only need to run this cell if using a word-embedding similarity metric\n",
    "# to run this cell, you will need to download the GoogleNewsVectors-negative300 word embeddings\n",
    "\n",
    "# Simple, order-dependent Word-Embedding based topic set similarity metric based on that proposed in \n",
    "# Wang, Xi. (2019). Evaluating Similarity Metrics for Latent Twitter Topics. \n",
    "\n",
    "# Aggregate topic-vector word embedding similarity metric also provided\n",
    "\n",
    "import gensim\n",
    "\n",
    "print(\"Loading Word2Vec model (this may take some time)...\")\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('../scratch/GoogleNewsVectors.gz', binary=True)  \n",
    "print(\"Word2Vec model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordVecOrZero(word):\n",
    "    # Get the vector for the word as a NP array, or return zero if not contained\n",
    "    if word in word2vec.vocab:\n",
    "        toReturn = word2vec[word]\n",
    "        return toReturn\n",
    "    else:\n",
    "        return np.zeros(word2vec.vector_size, dtype=np.float32)\n",
    "\n",
    "def getWordCosineSim(word1, word2):\n",
    "    # Get cosine similarity for word vectors, returning zero if either is not contained in the embedding vocab\n",
    "    wordVec1 = getWordVecOrZero(word1)\n",
    "    wordVec2 = getWordVecOrZero(word2)\n",
    "    return np.dot(wordVec1, wordVec2)\n",
    "    \n",
    "def getEmbeddingBasedSimScore(topicSet1, topicSet2):\n",
    "    # Get topic set similarity by pair-wise word embeddings\n",
    "    topicSetSim = 0.0\n",
    "    total = 0.0\n",
    "    for (topic1, topic2) in zip(topicSet1, topicSet2):\n",
    "        for ((word1, _) , (word2, _)) in zip(topic1, topic2):\n",
    "            topicSetSim += getWordCosineSim(word1, word2)\n",
    "            total += 1.0\n",
    "    return topicSetSim / total\n",
    "\n",
    "def getAggregateEmbeddingBasedSimScore(topicSet1, topicSet2):\n",
    "    # Aggregate each topic into a single vector (sum of words), normalize it, and then \n",
    "    # get that vector cosine similarity to the opposing topic and sum it up for a final set sim score\n",
    "    topicSetSim = 0.0\n",
    "    total = 0.0\n",
    "    for (topic1, topic2) in zip(topicSet1, topicSet2):\n",
    "        topicVec1 = np.zeros(word2vec.vector_size, dtype=np.float32)\n",
    "        topicVec2 = np.zeros(word2vec.vector_size, dtype=np.float32)\n",
    "        for ((word1, _) , (word2, _)) in zip(topic1, topic2):\n",
    "            topicVec1 += getWordVecOrZero(word1)\n",
    "            topicVec2 += getWordVecOrZero(word2)\n",
    "        topicVec1 = np.linalg.norm(topicVec1)\n",
    "        topicVec2 = np.linalg.norm(topicVec2)\n",
    "        topicSetSim += np.dot(topicVec1, topicVec2)\n",
    "        total += 1.0\n",
    "    return topicSetSim / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate topic set sims--note that this cell takes about 13 hours to run on Jaccard coefficients, 18 hours on WE\n",
    "# I provide a np.save() and np.load() for the sim scores array so that the notebook can be closed / reloaded with that data.\n",
    "\n",
    "def getJaccardTopicSetSim(topicSet1, topicSet2):\n",
    "    (imperfCoef, simScore) = getTopicSetSim(topicSet1, topicSet2)\n",
    "    imperCoef = 1 if imperfCoef is None else imperfCoef\n",
    "    return imperfCoef * simScore\n",
    "\n",
    "def getWordEmbeddingTopicSetSim(topicSet1, topicSet2):\n",
    "    # Uses the word-by-word approach proposed in Wang, Xi. (2019). Evaluating Similarity Metrics for Latent Twitter Topics. \n",
    "    return getEmbeddingBasedSimScore(topicSet1, topicSet2)\n",
    "\n",
    "def getTopicEmbeddingTopicSetSim(topicSet1, topicSet2):\n",
    "    # Uses aggregation of the topic into one normalized vector, then compares the vectors for each topic\n",
    "    return getAggregateEmbeddingBasedSimScore(topicSet1, topicSet2)\n",
    "    \n",
    "\n",
    "# CHANGE THIS FUNCTION to adjust between word-embedding topic similarity and Jaccard similarity\n",
    "def calcTopicSetSim(topicSet1, topicSet2):\n",
    "#     return getJaccardTopicSetSim(topicSet1, topicSet2)\n",
    "    return getAggregateEmbeddingBasedSimScore(topicSet1, topicSet2)\n",
    "#     return getWordEmbeddingTopicSetSim(topicSet1, topicSet2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Conversions for Performance</h3>\n",
    "\n",
    "One issue with the original `getTopicsForDocument(docID)` function is that it is extremely slow. Pandas just isn't able to perform large selections by field-matching the `doc_id` column on this large of a dataset.\n",
    "\n",
    "It turns out that a Python dictionary has the same problem--it doesn't rehash very efficiently at this size, so inserting stuff there gives performance that degrades so quickly that the below loop would never finish.\n",
    "\n",
    "So, I'll instead use a sparse NumPy array with object type to hold all of the topic lists. This conversion still takes some time--about 50it/s (a couple of hours total) but it makes calculating similarities exponentially faster later on.\n",
    "\n",
    "With that said: the below cell is optional. But if you want to use this code on a large dataset as intended, it will be of huge benefit to run this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#################################    OPTIONAL CELL     ################################\n",
    "# Run this cell to increase performance significantly on large datsets. See above note.\n",
    "#######################################################################################\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use a Numpy array to store these lists... it's far faster than dictionary for numeric keys\n",
    "documentTopics = np.empty(max(models.doc_id), dtype=np.object)\n",
    "\n",
    "documentIDs = pd.unique(models.doc_id)\n",
    "\n",
    "ITEMS_PER_DOC = 10 * 15 # 10 words for each of 15 topics\n",
    "\n",
    "counter = 0\n",
    "for docID in documentIDs:\n",
    "    # Get topics by unsafe row indexing, but use verification of expected doc ID (slightly slower)\n",
    "    documentTopics[docID] = getTopicsFromRows(counter * ITEMS_PER_DOC, (counter + 1) * ITEMS_PER_DOC, docID)\n",
    "    counter += 1\n",
    "        \n",
    "# Now redefine the getTopicsForDocument function for future use\n",
    "\n",
    "def getTopicsForDocument(docID):\n",
    "    return documentTopics[docID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "# Single-threaded similarity score generator\n",
    "\n",
    "maxID = max(models['doc_id'])\n",
    "\n",
    "for i in tqdm(range(maxID)):\n",
    "    docTopics = getTopicsForDocument(i)\n",
    "    simScores[i] = calcTopicSetSim(docTopics, promptTopicSet)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Processed\", i, \"similarity scores.\")\n",
    "\n",
    "np.save('simScores_array', simScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "simScores = np.load('simScores_array.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by highest sim scores\n",
    "full_corpus.sort_values('aggSimScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
