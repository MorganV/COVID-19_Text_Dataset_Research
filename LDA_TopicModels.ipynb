{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LDA Topic Modeling of CORD19 Data</h1>\n",
    "<h3>Morgan VandenBerg</h3>\n",
    "\n",
    "This notebook provides utilities to generate and explore topic models on the CORD19 dataset, found on Kaggle at:\n",
    "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/\n",
    "\n",
    "Here is a link to the `topic_models.csv` file that can be read in to avoid processing the first section of this notebook: https://smu.box.com/s/3t52kyb3jeeftd8q4w3zd9bl59ba6x9b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is a generic topic model builder class adapted from SMU's CS7391 Special Topics (Natural Language Processing) second homework project. Note that the `NUM_CORES` parameter should be left at 1 for the CORD19 data if processing individual paragraphs, as the multithreading overhead is too high to increase efficiency on these small documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Allows me to speed up tagging without using pos_tag_sents\n",
    "# as per https://stackoverflow.com/questions/33676526/pos-tagger-is-incredibly-slow\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import nltk\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "NUM_CORES = 1\n",
    "\n",
    "class LDAModelBuilder:\n",
    "    _stemLemmaTool = None\n",
    "    _stemDictionary = {}\n",
    "    _tagger = PerceptronTagger()\n",
    "\n",
    "    def __init__(self, numTopics, vectorModel, alpha, useToken, usePOS, useStemLemma, stopwordsFile, outputFile, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self._numTopics = numTopics\n",
    "        self._vectorModel = vectorModel\n",
    "        self._alpha = alpha\n",
    "        self._useToken = useToken\n",
    "        self._usePOS = usePOS\n",
    "        self._useStemLemma = useStemLemma\n",
    "        self.__initStopWords(stopwordsFile)\n",
    "        self._outputFile = outputFile\n",
    "\n",
    "    def __initStopWords(self, stopwordsFile):\n",
    "        if stopwordsFile == None:\n",
    "            if self.verbose:\n",
    "                print(\"\\tSelected 'none' for stopwords.\")\n",
    "            self._stopwords = set()\n",
    "        elif stopwordsFile == 'nltk':\n",
    "            if self.verbose:\n",
    "                print(\"\\tSelected NLTK stopwords.\")\n",
    "            self._stopwords = set(nltk_stopwords.words('english'))\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"\\tReading custom stopwords from file.\")\n",
    "            self._stopwords = set(line.strip().lower()\n",
    "                                  for line in open(stopwordsFile))\n",
    "\n",
    "    def getStopwordSet(self):\n",
    "        return self._stopwords\n",
    "\n",
    "    def __stemOrLemmatizeDocument(self, document):\n",
    "        # Stem or lemmatize document given program config\n",
    "        if self._useStemLemma == 'N':\n",
    "            return document\n",
    "        elif self._useStemLemma == 'B':\n",
    "            return self.__stemDocument(document)\n",
    "        elif self._useStemLemma == 'L':\n",
    "            return self.__lemmatizeDoc(document)\n",
    "        else:\n",
    "            print(\"Unsupported stem/lemmatization setting given in config file.\")\n",
    "\n",
    "    def __stemDocument(self, documentTokens):\n",
    "        # Stem documents using PorterStemmer\n",
    "        toStem = []\n",
    "        # process document to remove parts of speech as specified, since lemmatization function will do this automatically\n",
    "        partsSpeech = self._tagger.tag(documentTokens)\n",
    "        for word, tag in partsSpeech:\n",
    "            wntag = self.__getWordnetTag(tag)\n",
    "            if self.__keepPartOfSpeech(wntag):\n",
    "                toStem.append(word)\n",
    "\n",
    "        # Pass items through stemmer, memoizing / referencing dictionary for performance\n",
    "        toReturn = []\n",
    "        if self._stemLemmaTool is None:\n",
    "            self._stemLemmaTool = PorterStemmer()\n",
    "        for word in toStem:\n",
    "            if word not in self._stemDictionary:\n",
    "                self._stemDictionary[word] = self._stemLemmaTool.stem(word)\n",
    "            toReturn.append(self._stemDictionary[word])\n",
    "\n",
    "        return toReturn\n",
    "\n",
    "    def __lemmatizeDoc(self, documentTokens):\n",
    "        # Lemmatize the document tokens using NLTK pos_tag\n",
    "        toReturn = []\n",
    "        if self._stemLemmaTool is None:\n",
    "            self._stemLemmaTool = WordNetLemmatizer()\n",
    "        partsSpeech = self._tagger.tag(documentTokens)\n",
    "\n",
    "        for word, tag in partsSpeech:\n",
    "            wntag = self.__getWordnetTag(tag)\n",
    "            lemma = None\n",
    "            if self.__keepPartOfSpeech(wntag):\n",
    "                if wntag is None:\n",
    "                    lemma = self._stemLemmaTool.lemmatize(word)\n",
    "                else:\n",
    "                    lemma = self._stemLemmaTool.lemmatize(word, pos=wntag)\n",
    "                toReturn.append(lemma)\n",
    "        return toReturn\n",
    "\n",
    "    def __keepPartOfSpeech(self, pos):\n",
    "        # Determine if the word should be kept given its part of speech and program config\n",
    "        if self._usePOS == 'A':\n",
    "            return True\n",
    "        elif self._usePOS == 'F':\n",
    "            return (pos == wordnet.NOUN or pos == wordnet.VERB or pos == wordnet.ADJ or pos == wordnet.ADV)\n",
    "        elif self._usePOS == 'N':\n",
    "            return (pos == wordnet.NOUN or pos == wordnet.ADJ)\n",
    "        elif self._usePOS == 'n':\n",
    "            return (pos == wordnet.NOUN)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __getWordnetTag(self, tag):\n",
    "        # Convert to WordNet tags (from Penn)\n",
    "        # Source for this method: https://stackoverflow.com/a/15590384\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __isStopWord(self, word):\n",
    "        if word.lower() in self._stopwords:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __processWordToKeep(self, word):\n",
    "        if self._useToken == 'A':\n",
    "            # Keep all words except single character non-alphanumeric characters\n",
    "            if len(word) == 1 and re.search(r'\\W', word):\n",
    "                # Case for single-character nonalphanumeric\n",
    "                return None\n",
    "            else:\n",
    "                return word\n",
    "\n",
    "        elif self._useToken == 'a':\n",
    "            # Keep all words except single character non-alphanumeric characters,\n",
    "            # remove symbols if token is a mixture of alphanumeric and symbols\n",
    "            if len(word) == 1 and re.search(r'\\W', word):\n",
    "                # Case for single-character nonalphanumeric\n",
    "                return None\n",
    "            else:\n",
    "                return re.sub(r'\\W', '', word)\n",
    "\n",
    "        elif self._useToken == 'N':\n",
    "            # Keep only alphanumeric tokens\n",
    "            if re.search(r'\\W', word):\n",
    "                # Case for non-alphanumeric\n",
    "                return None\n",
    "            else:\n",
    "                # Valid case\n",
    "                return word\n",
    "\n",
    "        elif self._useToken == 'n':\n",
    "            # Keep only alphanumeric tokens, removing tokens that are only numbers\n",
    "            if re.search(r'\\W', word):\n",
    "                # Case for non-alphanumeric\n",
    "                return None\n",
    "            if not re.search(r'[a-zA-Z]', word):\n",
    "                # Case for only numbers\n",
    "                return None\n",
    "            else:\n",
    "                # Valid case\n",
    "                return word\n",
    "\n",
    "    def preProcessDocument(self, doc):\n",
    "        return self.__preProcessDocument(word_tokenize(doc))\n",
    "\n",
    "    def getBagOfWords(self, tokens):\n",
    "        return self.id2word_.doc2bow(tokens)\n",
    "\n",
    "    def __preProcessDocument(self, tokens):\n",
    "        # Perform stemming or lemmatization\n",
    "        firstPass = []\n",
    "        for word in tokens:\n",
    "            if len(word) < 3:\n",
    "                continue\n",
    "            word=word.lower()\n",
    "            keepWord = self.__processWordToKeep(word)\n",
    "            if keepWord is not None and not self.__isStopWord(keepWord):\n",
    "                firstPass.append(keepWord)\n",
    "        firstPass = self.__stemOrLemmatizeDocument(firstPass)\n",
    "        # Now we remove stop words again\n",
    "        toReturn = []\n",
    "        for word in firstPass:\n",
    "            if not self.__isStopWord(word):\n",
    "                toReturn.append(word)\n",
    "\n",
    "        return toReturn\n",
    "\n",
    "    def __buildGensimCorpus(self, documents):\n",
    "        if self.verbose:\n",
    "            print(\"\\tBuilding GenSim corpus...\")\n",
    "        # Build a GenSim corpus given documents\n",
    "        processedDocuments = []\n",
    "        for doc in documents:\n",
    "            processedDocuments.append(self.preProcessDocument(doc))\n",
    "\n",
    "        wordIDs = corpora.Dictionary(processedDocuments)\n",
    "        corpus = [wordIDs.doc2bow(text) for text in processedDocuments]\n",
    "\n",
    "        if self._vectorModel == 'B':\n",
    "            # Use binary model\n",
    "            for document in corpus:\n",
    "                document[:] = [(id, 1 if freq > 0 else 0)\n",
    "                               for (id, freq) in document]\n",
    "\n",
    "        elif self._vectorModel == 'T':\n",
    "            # Use TFIDF model\n",
    "            tfidf = gensim.models.TfidfModel(corpus)\n",
    "            corpus = tfidf[corpus]\n",
    "\n",
    "        # Else 't' use TF model (no adjustment)\n",
    "        elif not self._vectorModel == 't':\n",
    "            print(\"Unsupported vector model passed in config file.\")\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\tBuilt GenSim corpus.\")\n",
    "        return (corpus, wordIDs)\n",
    "\n",
    "    def __buildLDAModel(self, corpus, id2word):\n",
    "        return gensim.models.ldamulticore.LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            num_topics=self._numTopics,\n",
    "            alpha=self._alpha,\n",
    "            workers=NUM_CORES\n",
    "        )\n",
    "\n",
    "    def trainLDA(self, documents):\n",
    "        trainSuccess = False\n",
    "        if self.verbose:\n",
    "            print(\"\\tBuilding GenSim LDA topic model...\")\n",
    "        # Build / train LDA model via GenSim\n",
    "        corpus, wordIDs = self.__buildGensimCorpus(documents)\n",
    "        if len(wordIDs) > 0:\n",
    "            self.LDAmodel_ = self.__buildLDAModel(corpus, wordIDs)\n",
    "            self.id2word_ = wordIDs\n",
    "            self._corpus_ = corpus\n",
    "            trainSuccess = True\n",
    "            if self.verbose:\n",
    "                print(\"\\tBuilt GenSim LDA topic model.\")\n",
    "        return trainSuccess\n",
    "    \n",
    "    def saveModel_(self):\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaving LDA topic model...\")\n",
    "        # Output the model, note I use SKLearn convention with pre/post-underscore to denote pre/post train functions\n",
    "        self.LDAmodel_.save(self._outputFile + '.model')\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaved LDA topic model.\")\n",
    "\n",
    "    def loadModel(self, fromFile):\n",
    "        self.LDAmodel_ = gensim.models.ldamulticore.LdaMulticore.load(fromFile)\n",
    "            \n",
    "    def getTopic(self, topicID, n=10):\n",
    "        topic = self.LDAmodel_.get_topic_terms(topicid=topicID, topn=n)\n",
    "        # Transform word IDs back to the original word\n",
    "        topic[:] = [(self.id2word_[id], prob) for (id, prob) in topic]\n",
    "        return topic\n",
    "\n",
    "    def saveTopics_(self, n=10):\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaving LDA topics...\")\n",
    "        for topicID in range(0, self._numTopics):\n",
    "            topic = self.getTopic(topicID=topicID, n=n)\n",
    "            # Write topic output\n",
    "            with open(self._outputFile + '_' + str(topicID) + '.topic', 'w') as writer:\n",
    "                for (word, prob) in topic:\n",
    "                    writer.write(word)\n",
    "                    writer.write(' ')\n",
    "                    writer.write(str(prob))\n",
    "                    writer.write('\\n')\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaved LDA topics.\")\n",
    "\n",
    "    def generateAndSaveDocTopics_(self, fileNames):\n",
    "        if self.verbose:\n",
    "            print(\"\\tGenerating and saving document topics...\")\n",
    "        # Pass file names in as vector corresponding to original corpus documents\n",
    "        with open(self._outputFile + '.dt', 'w') as writer:\n",
    "            # Iterate over documents / filenames simultaneously\n",
    "            for document, fileName in zip(self._corpus_, fileNames):\n",
    "                writer.write(fileName)\n",
    "                writer.write(' ')\n",
    "                # Get topics for document\n",
    "                docTopics = self.LDAmodel_.get_document_topics(\n",
    "                    document, minimum_probability=0)\n",
    "                # Write document topics to file\n",
    "                for (topicID, prob) in docTopics:\n",
    "                    writer.write(str(prob))\n",
    "                    writer.write(' ')\n",
    "                writer.write('\\n')\n",
    "        print(\"\\tGenerated and saved document topics.\")\n",
    "\n",
    "\n",
    "def getJaccard(s, t):\n",
    "    # Given two sets of words\n",
    "    # Calculate the Jaccard coefficient | S ⋂ T | / | S ⋃ T |\n",
    "    numer = len(s.intersection(t))\n",
    "    denom = len(s.union(t))\n",
    "    return numer / denom if denom > 0 else 0\n",
    "\n",
    "\n",
    "def getTopicSim(t1, t2):\n",
    "    # Given two topics, get their similarity as Jaccard of T1(k) and T2(k)\n",
    "    # Format note: t1, t2 should be sets of tuple representing (topic, topic_prob)\n",
    "    t1_words = set()\n",
    "    t2_words = set()\n",
    "    for ((t1_word, _), (t2_word, _)) in zip(t1, t2):\n",
    "        t1_words.add(t1_word)\n",
    "        t2_words.add(t2_word)\n",
    "    sim = getJaccard(t1_words, t2_words)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def getTopicSetSim(tprime, uprime):\n",
    "    selectedUvals = set()  # Used to see if we get a perfect match\n",
    "    simSum = 0\n",
    "    for t in tprime:\n",
    "        bestTopic = None\n",
    "        bestSim = None\n",
    "        counter = 0\n",
    "        bestTopicIndex = 0\n",
    "        for u in uprime:\n",
    "            if bestTopic is None:\n",
    "                # First index: assign base best topic, sim, index\n",
    "                bestTopic = u\n",
    "                bestTopicIndex = 0\n",
    "                bestSim = getTopicSim(t, u)\n",
    "                continue\n",
    "            # Get similarity for current topic\n",
    "            sim = getTopicSim(t, u)\n",
    "            if sim > bestSim:\n",
    "                bestSim = sim\n",
    "                bestTopic = u\n",
    "                bestTopicIndex = counter\n",
    "            counter += 1\n",
    "        selectedUvals.add(bestTopicIndex)  # Selected U at index (counter)\n",
    "        simSum += bestSim\n",
    "    if len(selectedUvals) == len(tprime):\n",
    "        # Perfect match was found\n",
    "        return (None, simSum)\n",
    "    else:\n",
    "        # Did not find a perfect match\n",
    "        return (\n",
    "            # First term: number of selected topics / number of topics in T\n",
    "            len(selectedUvals) / len(tprime),\n",
    "            simSum\n",
    "        )\n",
    "\n",
    "\n",
    "def getWordnetTag(tag):\n",
    "    # Convert to WordNet tags (from Penn)\n",
    "    # Source for this method: https://stackoverflow.com/a/15590384\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord19Path = './../scratch/CORD19Data/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xqhn0vbp</td>\n",
       "      <td>1e1286db212100993d03cc22374b624f7caee956</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Airborne rhinovirus detection and effect of ul...</td>\n",
       "      <td>10.1186/1471-2458-3-5</td>\n",
       "      <td>PMC140314</td>\n",
       "      <td>12525263.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: Rhinovirus, the most common cause ...</td>\n",
       "      <td>2003-01-13</td>\n",
       "      <td>Myatt, Theodore A; Johnston, Sebastian L; Rudn...</td>\n",
       "      <td>BMC Public Health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gi6uaa83</td>\n",
       "      <td>8ae137c8da1607b3a8e4c946c07ca8bda67f88ac</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Discovering human history from stomach bacteria</td>\n",
       "      <td>10.1186/gb-2003-4-5-213</td>\n",
       "      <td>PMC156578</td>\n",
       "      <td>12734001.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Recent analyses of human pathogens have reveal...</td>\n",
       "      <td>2003-04-28</td>\n",
       "      <td>Disotell, Todd R</td>\n",
       "      <td>Genome Biol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>le0ogx1s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC</td>\n",
       "      <td>A new recruit for the army of the men of death</td>\n",
       "      <td>10.1186/gb-2003-4-7-113</td>\n",
       "      <td>PMC193621</td>\n",
       "      <td>12844350.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>The army of the men of death, in John Bunyan's...</td>\n",
       "      <td>2003-06-27</td>\n",
       "      <td>Petsko, Gregory A</td>\n",
       "      <td>Genome Biol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fy4w7xz8</td>\n",
       "      <td>0104f6ceccf92ae8567a0102f89cbb976969a774</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Association of HLA class I with severe acute r...</td>\n",
       "      <td>10.1186/1471-2350-4-9</td>\n",
       "      <td>PMC212558</td>\n",
       "      <td>12969506.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: The human leukocyte antigen (HLA) ...</td>\n",
       "      <td>2003-09-12</td>\n",
       "      <td>Lin, Marie; Tseng, Hsiang-Kuang; Trejaut, Jean...</td>\n",
       "      <td>BMC Med Genet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0qaoam29</td>\n",
       "      <td>5b68a553a7cbbea13472721cd1ad617d42b40c26</td>\n",
       "      <td>PMC</td>\n",
       "      <td>A double epidemic model for the SARS propagation</td>\n",
       "      <td>10.1186/1471-2334-3-19</td>\n",
       "      <td>PMC222908</td>\n",
       "      <td>12964944.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: An epidemic of a Severe Acute Resp...</td>\n",
       "      <td>2003-09-10</td>\n",
       "      <td>Ng, Tuen Wai; Turinici, Gabriel; Danchin, Antoine</td>\n",
       "      <td>BMC Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  xqhn0vbp  1e1286db212100993d03cc22374b624f7caee956      PMC   \n",
       "1  gi6uaa83  8ae137c8da1607b3a8e4c946c07ca8bda67f88ac      PMC   \n",
       "2  le0ogx1s                                       NaN      PMC   \n",
       "3  fy4w7xz8  0104f6ceccf92ae8567a0102f89cbb976969a774      PMC   \n",
       "4  0qaoam29  5b68a553a7cbbea13472721cd1ad617d42b40c26      PMC   \n",
       "\n",
       "                                               title                      doi  \\\n",
       "0  Airborne rhinovirus detection and effect of ul...    10.1186/1471-2458-3-5   \n",
       "1    Discovering human history from stomach bacteria  10.1186/gb-2003-4-5-213   \n",
       "2     A new recruit for the army of the men of death  10.1186/gb-2003-4-7-113   \n",
       "3  Association of HLA class I with severe acute r...    10.1186/1471-2350-4-9   \n",
       "4   A double epidemic model for the SARS propagation   10.1186/1471-2334-3-19   \n",
       "\n",
       "       pmcid   pubmed_id license  \\\n",
       "0  PMC140314  12525263.0   no-cc   \n",
       "1  PMC156578  12734001.0   no-cc   \n",
       "2  PMC193621  12844350.0   no-cc   \n",
       "3  PMC212558  12969506.0   no-cc   \n",
       "4  PMC222908  12964944.0   no-cc   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  BACKGROUND: Rhinovirus, the most common cause ...   2003-01-13   \n",
       "1  Recent analyses of human pathogens have reveal...   2003-04-28   \n",
       "2  The army of the men of death, in John Bunyan's...   2003-06-27   \n",
       "3  BACKGROUND: The human leukocyte antigen (HLA) ...   2003-09-12   \n",
       "4  BACKGROUND: An epidemic of a Severe Acute Resp...   2003-09-10   \n",
       "\n",
       "                                             authors            journal  \\\n",
       "0  Myatt, Theodore A; Johnston, Sebastian L; Rudn...  BMC Public Health   \n",
       "1                                   Disotell, Todd R        Genome Biol   \n",
       "2                                  Petsko, Gregory A        Genome Biol   \n",
       "3  Lin, Marie; Tseng, Hsiang-Kuang; Trejaut, Jean...      BMC Med Genet   \n",
       "4  Ng, Tuen Wai; Turinici, Gabriel; Danchin, Antoine     BMC Infect Dis   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN           True   \n",
       "1                          NaN            NaN           True   \n",
       "2                          NaN            NaN          False   \n",
       "3                          NaN            NaN           True   \n",
       "4                          NaN            NaN           True   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0               True  custom_license   \n",
       "1               True  custom_license   \n",
       "2               True  custom_license   \n",
       "3               True  custom_license   \n",
       "4               True  custom_license   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...  \n",
       "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_path = cord19Path + 'metadata.csv'\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code reads in the CORD19 metadata.\n",
    "\n",
    "The below code will read in the full dataset. Note that this takes a significant amount of time and consumes a large amount of memory. Moreover, it is NOT necessary to run other items in this notebook--only to generate the original topics, which takes ~30 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data loading code adapted from \n",
    "# https://github.com/elilaird/COVID-19-Open-Research-Dataset-Challenge/blob/master/CORD-19-Topic-Modeling.ipynb\n",
    "\n",
    "'''\n",
    "    @Desc    : Reads in json article and converts into Pandas Dataframe\n",
    "    @Params  : filepath (str)\n",
    "    @Returns : Pandas Dataframe \n",
    "'''\n",
    "def JsonToDataFrame(filepath):\n",
    "        \n",
    "    #read json into dict\n",
    "    with open(filepath) as json_data:\n",
    "        data = json.load(json_data)\n",
    "        \n",
    "        paper_id = data['paper_id']\n",
    "        abstract = '\\n'.join([section['text'] for section in data['abstract']])\n",
    "\n",
    "        \n",
    "\n",
    "        final_data = {\n",
    "            'paper_id'  : [data['paper_id']],\n",
    "            'section'   : ['abstract'],\n",
    "            'text'  : ['\\n'.join([section['text'] for section in data['abstract']])]                                       \n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(final_data)\n",
    "        for section in data['body_text']:\n",
    "            df = df.append({\n",
    "                'paper_id' : data['paper_id'],\n",
    "                'section'  : section['section'],\n",
    "                'text'     : section['text']\n",
    "            }, ignore_index=True)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "        \n",
    "biorxiv_medrxiv    = cord19Path + 'biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/'\n",
    "comm_use_subset    = cord19Path + 'comm_use_subset/comm_use_subset/pdf_json/'\n",
    "noncomm_use_subset = cord19Path + 'noncomm_use_subset/noncomm_use_subset/pdf_json/'\n",
    "\n",
    "biorxiv_medrxiv_files       = [biorxiv_medrxiv + pos_json for pos_json in os.listdir(biorxiv_medrxiv) if pos_json.endswith('.json')]\n",
    "comm_use_subset_files       = [comm_use_subset + pos_json for pos_json in os.listdir(comm_use_subset) if pos_json.endswith('.json')]\n",
    "noncomm_use_subset_files    = [noncomm_use_subset + pos_json for pos_json in os.listdir(noncomm_use_subset) if pos_json.endswith('.json')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of files with issues: 0\n",
      "CPU times: user 1min 49s, sys: 15.8 s, total: 2min 5s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Corpus loading code from https://github.com/elilaird/COVID-19-Open-Research-Dataset-Challenge/blob/clay_bert/CORD-19_BERT_clustering.ipynb\n",
    "# adapted with multiprocessing by Clay Harper\n",
    "\n",
    "import concurrent\n",
    "\n",
    "\n",
    "#initialize dfs\n",
    "biomed_df      = pd.DataFrame()\n",
    "comm_use_df    = pd.DataFrame()\n",
    "noncomm_use_df = pd.DataFrame()\n",
    "\n",
    "count = 0\n",
    "\n",
    "def to_df(file):\n",
    "    try:\n",
    "        return JsonToDataFrame(file)\n",
    "    except:\n",
    "        global count \n",
    "        count += 1\n",
    "        return None\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for _, df in zip(biorxiv_medrxiv_files, executor.map(to_df, biorxiv_medrxiv_files)):\n",
    "            if df is not None:\n",
    "                biomed_df = biomed_df.append(df, ignore_index=True)\n",
    "            \n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for _, df in zip(comm_use_subset_files, executor.map(to_df, comm_use_subset_files)):\n",
    "            if df is not None:\n",
    "                comm_use_df = comm_use_df.append(df, ignore_index=True)\n",
    "            \n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for _, df in zip(noncomm_use_subset_files, executor.map(to_df, noncomm_use_subset_files)):\n",
    "            if df is not None:\n",
    "                noncomm_use_df = noncomm_use_df.append(df, ignore_index=True)\n",
    "            \n",
    "print('Count of files with issues: {}'.format(count))\n",
    "full_corpus = pd.concat([biomed_df, comm_use_df, noncomm_use_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62b978569b93bcbafae6ae2a082dc70b8c6abdf0</td>\n",
       "      <td>abstract</td>\n",
       "      <td>we use human mobility models for which we are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62b978569b93bcbafae6ae2a082dc70b8c6abdf0</td>\n",
       "      <td></td>\n",
       "      <td>1 complete lockdown works about 10 days after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62b978569b93bcbafae6ae2a082dc70b8c6abdf0</td>\n",
       "      <td></td>\n",
       "      <td>5 in our simulations removal of infections at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62b978569b93bcbafae6ae2a082dc70b8c6abdf0</td>\n",
       "      <td></td>\n",
       "      <td>evidently all of these results have to be take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62b978569b93bcbafae6ae2a082dc70b8c6abdf0</td>\n",
       "      <td></td>\n",
       "      <td>what was not investigated in detail in our sim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id   section  \\\n",
       "0  62b978569b93bcbafae6ae2a082dc70b8c6abdf0  abstract   \n",
       "1  62b978569b93bcbafae6ae2a082dc70b8c6abdf0             \n",
       "2  62b978569b93bcbafae6ae2a082dc70b8c6abdf0             \n",
       "3  62b978569b93bcbafae6ae2a082dc70b8c6abdf0             \n",
       "4  62b978569b93bcbafae6ae2a082dc70b8c6abdf0             \n",
       "\n",
       "                                                text  \n",
       "0  we use human mobility models for which we are ...  \n",
       "1  1 complete lockdown works about 10 days after ...  \n",
       "2  5 in our simulations removal of infections at ...  \n",
       "3  evidently all of these results have to be take...  \n",
       "4  what was not investigated in detail in our sim...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "#remove punctuation\n",
    "full_corpus['text'] = full_corpus['text'].map(lambda x: x.translate(punct_table))\n",
    "\n",
    "#convert to lowercase\n",
    "full_corpus['text'] = full_corpus['text'].map(lambda x: x.lower())\n",
    "\n",
    "full_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449884"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_corpus)  # number of documents is almost 500k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Topic Models Exploration</h1>\n",
    "\n",
    "I begin by creating a topic model for the 'prompt'; the thing that we want to know more about. Note that using a moderate-sized document here will help with the stability of the model. One should not run topic models for individual sentences in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seek to find groups of topics that match this prompt\n",
    "\n",
    "promptText = '''\n",
    "What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface?\n",
    "Specifically, we want to know what the literature reports about:\n",
    "Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\n",
    "Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\n",
    "Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.\n",
    "Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\n",
    "Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\n",
    "Experimental infections to test host range for this pathogen.\n",
    "Animal host(s) and any evidence of continued spill-over to humans\n",
    "Socioeconomic and behavioral risk factors for this spill-over\n",
    "Sustainable risk reduction strategies\n",
    "'''\n",
    "\n",
    "promptDocs = [promptText]\n",
    "\n",
    "baseModelOutputPath = '../scratch/CORD19_Topic_Models/'\n",
    "\n",
    "ldaParams = dict(\n",
    "    numTopics = 15,           # 15 topics\n",
    "    vectorModel = 't',        # term freq detection model\n",
    "    alpha = 5,                # higher alpha for sharp topic detection\n",
    "    useToken = 'n',           # strictest token filtering \n",
    "    usePOS = 'N',             # nouns or adv \n",
    "    useStemLemma = 'L',       # lemmatization\n",
    "    stopwordsFile = 'nltk',   # LDA stopwords\n",
    "    outputFile = baseModelOutputPath + 'prompt_base_model',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "topicsToMatch = LDAModelBuilder(\n",
    "    **ldaParams\n",
    ")\n",
    "\n",
    "topicsToMatch.trainLDA(promptDocs)\n",
    "topicsToMatch.saveTopics_()\n",
    "\n",
    "k = 15 # number of words per topic\n",
    "promptTopicSet = []\n",
    "for i in range(0, ldaParams['numTopics']):\n",
    "    topic = topicsToMatch.getTopic(topicID=i, n=k)\n",
    "    promptTopicSet.append(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generating CORD19 Topic Models</h1>\n",
    "\n",
    "The two below cells are used to build the full dataset of topic models. Please note that these cells take an incredibly long time to run--most users should instead skip this section and use the `topic_models.csv` file that I make openly available through SMU's Box service.\n",
    "\n",
    "The first cell is a single-threaded approach to generating models sequentially, the bottom cell will run a specified number of processes and split the work between them. \n",
    "\n",
    "NOTE: the workflow here requires that the generated topics are written to disk. There will be close to two million total files here; consider this before running the below code. I performed all work with these on SMU's supercomputer, ManeFrame II (M2). Once the models are generated, you MUST use the `TopicModelCSVGenerator.java` or `TopicModelCSVGeneratorThreaded.java` programs to convert the individual model files into a single CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Now get topics for each document\n",
    "ldaModels = dict()\n",
    "\n",
    "numEmpty=0\n",
    "\n",
    "minDocLength = 10\n",
    "\n",
    "for i in range(len(full_corpus)):\n",
    "    docText = [full_corpus['text'].iloc[i]]\n",
    "    ldaParams['outputFile'] = baseModelOutputPath + '_model_doc_' + str(i)\n",
    "    ldaModels[i] = LDAModelBuilder(**ldaParams)\n",
    "    if ldaModels[i].trainLDA(docText):\n",
    "        # Save if train successful\n",
    "        ldaModels[i].saveTopics_()\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        \n",
    "print(\"Processed\", i, \"documents.\")\n",
    "print(\"Found\", numEmpty,\"empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multiprocessing for above code\n",
    "from multiprocessing import Pool, Lock, Process\n",
    "from multiprocessing.sharedctypes import Array\n",
    "\n",
    "\n",
    "ldaModels = dict()\n",
    "\n",
    "numEmpty=0\n",
    "\n",
    "minDocLength = 10\n",
    "\n",
    "\n",
    "def processLDA(lowerIndex, upperIndex, documents):\n",
    "    for (doc, i) in zip(documents, range(lowerIndex, upperIndex)):\n",
    "        docText = [doc]\n",
    "        ldaParams['outputFile'] = baseModelOutputPath + '_model_doc_' + str(i)\n",
    "        model = LDAModelBuilder(**ldaParams)\n",
    "        if model.trainLDA(docText):\n",
    "            model.saveTopics_()\n",
    "\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    numCores = 36\n",
    "    totalDocs = len(full_corpus)\n",
    "\n",
    "    processes = []\n",
    "    splits = np.array_split(full_corpus, numCores)\n",
    "    \n",
    "    lower = 0\n",
    "    \n",
    "    for i in range(numCores):\n",
    "        # spawn process with docs\n",
    "        docs = splits[i].text.tolist()\n",
    "        p = Process(target=processLDA, args=(lower, lower+len(docs), docs))\n",
    "        p.start()\n",
    "        print(\"Started process\", i)\n",
    "        processes.append(p)\n",
    "\n",
    "        lower += len(docs)\n",
    "\n",
    "        \n",
    "        \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        print(\"Joined process.\")\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"Processed\", i, \"documents.\")\n",
    "    print(\"Found\", numEmpty,\"empty.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell provides a Pythononic way to read in the models. However, on most systems (including SMU's supercomputer), it is too inefficient to run. One should instead use the provided Java code if the individual topics must be read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in calculated topic models\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "import numpy\n",
    "\n",
    "ldaDocumentTopics = numpy.empty(500000, dtype=object)\n",
    "print(\"alloc array\")\n",
    "\n",
    "numFailed = 0\n",
    "\n",
    "numTopics = 15\n",
    "\n",
    "for i in range(500000):\n",
    "    readFile = baseModelOutputPath + '_model_doc_' + str(i)\n",
    "    \n",
    "    ldaDocumentTopics[i] = list()\n",
    "    \n",
    "    for t in range(numTopics):\n",
    "        topics = []\n",
    "        #if path.exists(readFile + '_' + str(t) + '.topic'):\n",
    "        try:\n",
    "            with open(readFile + '_' + str(t) + '.topic', 'r') as topicFile:\n",
    "                for topicLine in topicFile:\n",
    "                    items = topicLine.split()\n",
    "                    topicString = items[0]\n",
    "                    topicWeight = float(items[1])\n",
    "                    topics.append((topicString, topicWeight))\n",
    "        except OSError:\n",
    "            pass\n",
    "        ldaDocumentTopics[i].append(topics)      \n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"read in \", i, \"models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Exploring Topic Models</h1>\n",
    "\n",
    "This is the section that most users should skip to! Be sure to download the `topic_models.csv` file and place it in the appropriate directory before using the below code. To calculate topic similarity scores, you can process this to a NumPy array (code below marked \"optional\") for a massive speedup. Or, you can instead download the `npy` file that I provide on Box and skip to the third \"optional\" cell.\n",
    "\n",
    "Each line of that CSV corresponds to a word and weight from an individual topic. That belongs to one of 15 topics for any individual document. The `doc_id` field is ordered identically to the `full_corpus` object read in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>expert</td>\n",
       "      <td>0.048942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dynamic</td>\n",
       "      <td>0.046465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>result</td>\n",
       "      <td>0.045898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>license</td>\n",
       "      <td>0.044178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>current</td>\n",
       "      <td>0.043589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  topic_id     word    weight\n",
       "0       0         0   expert  0.048942\n",
       "1       0         0  dynamic  0.046465\n",
       "2       0         0   result  0.045898\n",
       "3       0         0  license  0.044178\n",
       "4       0         0  current  0.043589"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read saved topic models from disk\n",
    "import pandas as pd\n",
    "models = pd.read_csv('topic_models_mthread.csv')\n",
    "models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.696855e+07</td>\n",
       "      <td>6.696855e+07</td>\n",
       "      <td>6.696855e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.248791e+05</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>4.915377e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.297174e+05</td>\n",
       "      <td>4.320494e+00</td>\n",
       "      <td>4.923616e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.126180e+05</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>2.631349e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.248230e+05</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>3.767874e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.371080e+05</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>5.726257e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.498830e+05</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             doc_id      topic_id        weight\n",
       "count  6.696855e+07  6.696855e+07  6.696855e+07\n",
       "mean   2.248791e+05  7.000000e+00  4.915377e-02\n",
       "std    1.297174e+05  4.320494e+00  4.923616e-02\n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00\n",
       "25%    1.126180e+05  3.000000e+00  2.631349e-02\n",
       "50%    2.248230e+05  7.000000e+00  3.767874e-02\n",
       "75%    3.371080e+05  1.100000e+01  5.726257e-02\n",
       "max    4.498830e+05  1.400000e+01  1.000000e+00"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Topic Similarity Matching -- Preparation</h2>\n",
    "\n",
    "I begin by finding paragraphs which match the prompt topics. The first approach below uses a simple Jaccard-coefficient based similarity measure. I will also explore techniques based on word-embeddings in later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# getTopicsForDocument will deterministically give the topics for a certain doc. But it is very, very slow. (10its/second)\n",
    "def getTopicsForDocument(docID):\n",
    "    topics = []\n",
    "    docTopics = models[ models.doc_id == docID ]\n",
    "    \n",
    "    for topicID in range(0, ldaParams['numTopics']):\n",
    "        topicItems = []\n",
    "        docTopic = docTopics[ docTopics.topic_id == topicID ]\n",
    "        for (topic, weight) in zip(docTopic.word.tolist(), docTopic.weight.tolist()):\n",
    "            topicItems.append( (topic, weight) )\n",
    "        topics.append(topicItems)\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# I can instead get topics very quickly if I index by rows\n",
    "def getTopicsFromRows(lower, upper, expectedID = None):\n",
    "    docTopics = models.iloc[lower:upper, :]\n",
    "    if expectedID is not None:\n",
    "        expectedLen = len(docTopics)\n",
    "        docTopics = docTopics[ docTopics.doc_id == expectedID ]\n",
    "        if len(docTopics) != expectedLen:\n",
    "            print(\"WARNING: at expected doc ID\",expectedID,\n",
    "                  \", number of items not matching expected ID:\",len(docTopics)-expectedLen)\n",
    "            \n",
    "    topics = []\n",
    "    for topicID in range(0, ldaParams['numTopics']):\n",
    "        topicItems = []\n",
    "        docTopic = docTopics[ docTopics.topic_id == topicID ]\n",
    "        for (topic, weight) in zip(docTopic.word.tolist(), docTopic.weight.tolist()):\n",
    "            topicItems.append( (topic, weight) )\n",
    "        topics.append(topicItems)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model (this may take some time)...\n",
      "Word2Vec model loaded.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: you only need to run this cell if using a word-embedding similarity metric\n",
    "# to run this cell, you will need to download the GoogleNewsVectors-negative300 word embeddings\n",
    "\n",
    "# Simple, order-dependent Word-Embedding based topic set similarity metric based on that proposed in \n",
    "# Wang, Xi. (2019). Evaluating Similarity Metrics for Latent Twitter Topics. \n",
    "\n",
    "# Aggregate topic-vector word embedding similarity metric also provided\n",
    "\n",
    "import gensim\n",
    "\n",
    "print(\"Loading Word2Vec model (this may take some time)...\")\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('../scratch/GoogleNewsVectors.gz', binary=True)  \n",
    "print(\"Word2Vec model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordVecOrZero(word):\n",
    "    # Get the vector for the word as a NP array, or return zero if not contained\n",
    "    if word in word2vec.vocab:\n",
    "        toReturn = word2vec[word]\n",
    "        return toReturn\n",
    "    else:\n",
    "        return np.zeros(word2vec.vector_size, dtype=np.float32)\n",
    "\n",
    "def getWordCosineSim(word1, word2, weight1=1.0, weight2=1.0):\n",
    "    # Get cosine similarity for word vectors, returning zero if either is not contained in the embedding vocab\n",
    "    wordVec1 = weight1 * getWordVecOrZero(word1)\n",
    "    wordVec2 = weight2 * getWordVecOrZero(word2)\n",
    "    return np.dot(wordVec1, wordVec2)\n",
    "    \n",
    "def getEmbeddingBasedSimScore(topicSet1, topicSet2, useTopicWeights=False):\n",
    "    # Get topic set similarity by pair-wise word embeddings\n",
    "    topicSetSim = 0.0\n",
    "    total = 0.0\n",
    "    for (topic1, topic2) in zip(topicSet1, topicSet2):\n",
    "        for ((word1, weight1) , (word2, weight2)) in zip(topic1, topic2):\n",
    "            if useTopicWeights:\n",
    "                topicSetSim += getWordCosineSim(word1, word2, weight1, weight2)\n",
    "            else:\n",
    "                topicSetSim += getWordCosineSim(word1, word2)\n",
    "            total += 1.0\n",
    "    return topicSetSim / total\n",
    "\n",
    "def getAggregateEmbeddingBasedSimScore(topicSet1, topicSet2, useTopicWeights=False):\n",
    "    # Aggregate each topic into a single vector (sum of words), normalize it, and then \n",
    "    # get that vector cosine similarity to the opposing topic and sum it up for a final set sim score\n",
    "    topicSetSim = 0.0\n",
    "    total = 0.0\n",
    "    for (topic1, topic2) in zip(topicSet1, topicSet2):\n",
    "        topicVec1 = np.zeros(word2vec.vector_size, dtype=np.float32)\n",
    "        topicVec2 = np.zeros(word2vec.vector_size, dtype=np.float32)\n",
    "        for ((word1, weight1) , (word2, weight2)) in zip(topic1, topic2):\n",
    "            if useTopicWeights:\n",
    "                weight1 = 1.0\n",
    "                weight2 = 1.0\n",
    "            topicVec1 += getWordVecOrZero(word1)\n",
    "            topicVec2 += getWordVecOrZero(word2)\n",
    "        topicVec1 = np.linalg.norm(topicVec1)\n",
    "        topicVec2 = np.linalg.norm(topicVec2)\n",
    "        topicSetSim += np.dot(topicVec1, topicVec2)\n",
    "        total += 1.0\n",
    "    return topicSetSim / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate topic set sims--note that this cell takes about 13 hours to run on Jaccard coefficients, 18 hours on WE\n",
    "# I provide a np.save() and np.load() for the sim scores array so that the notebook can be closed / reloaded with that data.\n",
    "\n",
    "def getJaccardTopicSetSim(topicSet1, topicSet2):\n",
    "    (imperfCoef, simScore) = getTopicSetSim(topicSet1, topicSet2)\n",
    "    imperCoef = 1 if imperfCoef is None else imperfCoef\n",
    "    return imperfCoef * simScore\n",
    "\n",
    "def getWordEmbeddingTopicSetSim(topicSet1, topicSet2, useTopicWeights=False):\n",
    "    # Uses the word-by-word approach proposed in Wang, Xi. (2019). Evaluating Similarity Metrics for Latent Twitter Topics. \n",
    "    return getEmbeddingBasedSimScore(topicSet1, topicSet2, useTopicWeights)\n",
    "\n",
    "def getTopicEmbeddingTopicSetSim(topicSet1, topicSet2, useTopicWeights=False):\n",
    "    # Uses aggregation of the topic into one normalized vector, then compares the vectors for each topic\n",
    "    return getAggregateEmbeddingBasedSimScore(topicSet1, topicSet2, useTopicWeights)\n",
    "    \n",
    "\n",
    "# CHANGE THIS FUNCTION to adjust between word-embedding topic similarity and Jaccard similarity\n",
    "def calcTopicSetSim(topicSet1, topicSet2):\n",
    "#     return getJaccardTopicSetSim(topicSet1, topicSet2)\n",
    "    return getAggregateEmbeddingBasedSimScore(topicSet1, topicSet2)\n",
    "#     return getWordEmbeddingTopicSetSim(topicSet1, topicSet2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Conversions for Performance</h3>\n",
    "\n",
    "One issue with the original `getTopicsForDocument(docID)` function is that it is extremely slow. Pandas just isn't able to perform large selections by field-matching the `doc_id` column on this large of a dataset.\n",
    "\n",
    "It turns out that a Python dictionary has the same problem--it doesn't rehash very efficiently at this size, so inserting stuff there gives performance that degrades so quickly that the below loop would never finish.\n",
    "\n",
    "So, I'll instead use a sparse NumPy array with object type to hold all of the topic lists. This conversion still takes some time--about 50it/s (a couple of hours total) but it makes calculating similarities exponentially faster later on.\n",
    "\n",
    "With that said: the below cell is optional. But if you want to use this code on a large dataset as intended, it will be of huge benefit to run this first.\n",
    "\n",
    "It also supports saving the NumPy array to disk an reading it in later. This saves significant training time. Please download the `docTopicsProcessed.npy` file that I make available via SMU Box and skip to the third cell marked \"OPTIONAL\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 30min 44s, sys: 58.2 s, total: 2h 31min 42s\n",
      "Wall time: 2h 31min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#################################    OPTIONAL CELL     ################################\n",
    "# Run this cell to increase performance significantly on large datsets. See above note.\n",
    "#######################################################################################\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use a Numpy array to store these lists... it's far faster than dictionary for numeric keys\n",
    "documentTopics = np.empty(max(models.doc_id) + 1, dtype=np.object)\n",
    "\n",
    "documentIDs = pd.unique(models.doc_id)\n",
    "\n",
    "ITEMS_PER_DOC = 10 * 15 # 10 words for each of 15 topics\n",
    "\n",
    "counter = 0\n",
    "for docID in documentIDs:\n",
    "    # Get topics by unsafe row indexing, but use verification of expected doc ID (slightly slower)\n",
    "    documentTopics[docID] = getTopicsFromRows(counter * ITEMS_PER_DOC, (counter + 1) * ITEMS_PER_DOC, docID)\n",
    "    counter += 1\n",
    "        \n",
    "# Now redefine the getTopicsForDocument function for future use\n",
    "\n",
    "def getTopicsForDocument(docID):\n",
    "    return documentTopics[docID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################    OPTIONAL CELL     ################################\n",
    "# Run this cell to save the output from the above cell to disk\n",
    "#######################################################################################\n",
    "np.save('docTopicsProcessed', documentTopics)\n",
    "\n",
    "def getTopicsForDocument(docID):\n",
    "    return documentTopics[docID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################    OPTIONAL CELL     ################################\n",
    "# Run this cell to read the processed topics from above cells from disk\n",
    "#######################################################################################\n",
    "documentTopics = np.load('docTopicsProcessed.npy', allow_pickle=True)\n",
    "\n",
    "def getTopicsForDocument(docID):\n",
    "    return documentTopics[docID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Topic Similarity Matching</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 14s, sys: 5.71 s, total: 29min 20s\n",
      "Wall time: 29min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Want to calculate all sim scores? Use this cell\n",
    "from tqdm import tqdm\n",
    "# Similarity score generator\n",
    "\n",
    "# Jaccard sim scores\n",
    "simScoresJaccard = np.zeros((len(documentTopics), 1))\n",
    "# Word-by-word embedding sim scores\n",
    "simScoresWE = np.zeros((len(documentTopics), 1))\n",
    "# Topic embedding sim scores\n",
    "simScoresTE = np.zeros((len(documentTopics), 1))\n",
    "\n",
    "for i in range(len(documentTopics)):\n",
    "    docTopics = getTopicsForDocument(i)\n",
    "    if docTopics is not None:\n",
    "        simScoresJaccard[i] = getJaccardTopicSetSim(docTopics, promptTopicSet)\n",
    "        simScoresWE[i] = getWordEmbeddingTopicSetSim(docTopics, promptTopicSet)\n",
    "        simScoresTE[i] = getTopicEmbeddingTopicSetSim(docTopics, promptTopicSet)\n",
    "    else:\n",
    "        simScoresJaccard[i] = 0.0\n",
    "        simScoresWE[i] = 0.0\n",
    "        simScoresTE[i] = 0.0\n",
    "\n",
    "np.save('simScoresJaccard', simScoresJaccard)\n",
    "np.save('simScoresWE', simScoresWE)\n",
    "np.save('simScoresTE', simScoresTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "# Single-metric similarity score generator\n",
    "simScores = np.zeros((len(documentTopics), 1))\n",
    "\n",
    "for i in tqdm(range(len(documentTopics))):\n",
    "    docTopics = getTopicsForDocument(i)\n",
    "    if docTopics is not None:\n",
    "        simScores[i] = calcTopicSetSim(docTopics, promptTopicSet)\n",
    "    else:\n",
    "        simScores[i] = 0.0\n",
    "\n",
    "np.save('simScores_array', simScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sim scores arrays\n",
    "simScoresJaccard = np.load('simScoresJaccard.npy')\n",
    "simScoresWE = np.load('simScoresWE.npy')\n",
    "simScoresTE = np.load('simScoresTE.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single simScore array\n",
    "simScores = np.load('simScores_array.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sim scores to full_corpus\n",
    "full_corpus['simScoresJaccard'] = simScoresJaccard\n",
    "full_corpus['simScoresWE'] = simScoresWE\n",
    "full_corpus['simScoresTE'] = simScoresTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreSortedCorpus = full_corpus.sort_values(['simScoresJaccard', 'simScoresTE', 'simScoresWE'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "      <th>simScores</th>\n",
       "      <th>simScoresJaccard</th>\n",
       "      <th>simScoresWE</th>\n",
       "      <th>simScoresTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59700</th>\n",
       "      <td>b84e5b51b40b345b68445d68483cc478e9ce5beb</td>\n",
       "      <td>Author Summary</td>\n",
       "      <td>the identification of influenza viruslike sequ...</td>\n",
       "      <td>206.524189</td>\n",
       "      <td>0.688453</td>\n",
       "      <td>1.316010</td>\n",
       "      <td>206.524189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48516</th>\n",
       "      <td>3bf6368f164189524748aa07f1a42fe808841b64</td>\n",
       "      <td>Target Amplification</td>\n",
       "      <td>target pcr is an effective and simple alternat...</td>\n",
       "      <td>210.746461</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.329268</td>\n",
       "      <td>210.746461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237327</th>\n",
       "      <td>e784769ed85685121bda82e89183d0b756686281</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>studies of humandisease epidemics have recentl...</td>\n",
       "      <td>182.176211</td>\n",
       "      <td>0.552460</td>\n",
       "      <td>1.039335</td>\n",
       "      <td>182.176211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17968</th>\n",
       "      <td>0f52f6ec3e15cc1cc45803c3cb81618d5ee82b7a</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>studies on agespecific epidemic activity curve...</td>\n",
       "      <td>213.234636</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>1.296840</td>\n",
       "      <td>213.234636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23246</th>\n",
       "      <td>d8a96ab6656a38b3c9380673aefaed8784982a11</td>\n",
       "      <td>AbstrACt</td>\n",
       "      <td>open access of affected medical institutions 7...</td>\n",
       "      <td>181.924909</td>\n",
       "      <td>0.531590</td>\n",
       "      <td>1.315190</td>\n",
       "      <td>181.924909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        paper_id               section  \\\n",
       "59700   b84e5b51b40b345b68445d68483cc478e9ce5beb        Author Summary   \n",
       "48516   3bf6368f164189524748aa07f1a42fe808841b64  Target Amplification   \n",
       "237327  e784769ed85685121bda82e89183d0b756686281          Introduction   \n",
       "17968   0f52f6ec3e15cc1cc45803c3cb81618d5ee82b7a          Introduction   \n",
       "23246   d8a96ab6656a38b3c9380673aefaed8784982a11              AbstrACt   \n",
       "\n",
       "                                                     text   simScores  \\\n",
       "59700   the identification of influenza viruslike sequ...  206.524189   \n",
       "48516   target pcr is an effective and simple alternat...  210.746461   \n",
       "237327  studies of humandisease epidemics have recentl...  182.176211   \n",
       "17968   studies on agespecific epidemic activity curve...  213.234636   \n",
       "23246   open access of affected medical institutions 7...  181.924909   \n",
       "\n",
       "        simScoresJaccard  simScoresWE  simScoresTE  \n",
       "59700           0.688453     1.316010   206.524189  \n",
       "48516           0.666667     1.329268   210.746461  \n",
       "237327          0.552460     1.039335   182.176211  \n",
       "17968           0.544118     1.296840   213.234636  \n",
       "23246           0.531590     1.315190   181.924909  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoreSortedCorpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the identification of influenza viruslike sequences in two different bat species has generated great interest in understanding their biology ability to mix with other influenza viruses and their public health threat unfortunately batinfluenza viruses couldnt be cultured from the samples containing the influenzalike nucleic acids we used synthetic genomics strategies to create wild type batinfluenza or batinfluenza modified by substituting the surface glycoproteins with those of model influenza a viruses although influenza viruslike particles were produced from both synthetic genomes only the modified batinfluenza viruses could be cultured the modified batinfluenza viruses replicated efficiently in vitro and an h1n1 modified version caused severe disease in mice collectively our data show 1 the two batflu genomes identified in other studies are replication competent suggesting that host cell specificity is the major limitation for propagation of batinfluenza 2 batinfluenza ns1 antagonizes host interferon response more efficiently than that of a model influenza a virus 3 batinfluenza has both genetic and protein incompatibility with influenza a or b viruses and 4 that these batinfluenza lineages pose little pandemic threat\n",
      "\n",
      "target pcr is an effective and simple alternative to metagenomics working well for both individual genes and small viral genomes 127  pcrbased amplification is often used for whole viral genome sequencing of samples with low viral load 54  eg during the investigation of the measles outbreak during the olympics in 2010 128 and epidemics of ebola 56 and zika diseases 129  sequencing of long 2530 kb amplified fragments has clarified the variability of a norovirus genome and its spreading among patients in a few hospitals in vietnam 130 131  a similar approach has been utilized to measure the specificity and sensitivity of the illumina platform for detection of minor polymorphisms in mixed hiv populations 132  deep sequencing of pcramplified viral genomes yielded complete genome sequences for the influenza virus 133  dengue virus 134 and hcv 135 \n",
      "\n",
      "studies of humandisease epidemics have recently begun to incorporate human reactions 14  most of these studies point to the existence of preventive responses that can reduce the spread of the infection such as the voluntary use of vaccines or facemasks that reduce the risk of being infected 15  in the field of animal diseases the human factor has received less attention however the role of biosecurity measures on the spread of animal diseases has been analyzed taking into account the externalities and strategic behavior of farmers 16 17  among english and welsh cattle farmers it has been shown that behavioral changes have an impact on the implementation of diseasecontrol programs 18  in asian countries misbehavior by farmers lack of perfect compliance with regulation and the emergence of underground poultry markets have been recognized as possible consequences of implementing control strategies 19  but their impacts have not been quantified\n",
      "\n",
      "studies on agespecific epidemic activity curves could provide key evidence on the mechanism of virus transmission and could facilitate the formulation of agespecific control measures 9 10  there is a growing body of evidence supporting that human mobility may drive the dispersal of influenza virus activity 11 12 13  however this hypothesis has not been supported consistently for example some studies 14 15 16 questioned the hypothesis that the children actually bring about influenza epidemic fluctuations the objective of this study is to identify the age groups in which influenza activity first peaks in the community using our extensive surveillance of ili cases over eight years and thereby fill a gap in the data available for temperate developing regions\n",
      "\n",
      "open access of affected medical institutions 7 due to increased public anxiety about merscov the trust in the korean government had fallen and the image of the korean president as a leader had been damaged 8 9 during contagious disease epidemics perceived risk can have a significant impact on precautionary behaviours that might affect disease transmission 10 11 12 a relevant empirical study emphasised that informing public about the disease outbreak such as the ebola virus could reduce worry about contracting the virus and take more preventive measures 13 the evaluation of public risk perception of disease helps us to know what knowledge the public needs therefore understanding the characteristics of risk perception and factors relating to how people perceive the risk is important in terms of minimising the impact of spread of infectious disease\n",
      "\n",
      "in recent years wsd has received significant scientific attention driven by the commercial impacts of this disease on shrimp global research efforts into understanding the biology of wssv and infection process for wsd have contributed to the formulation of strategies for disease treatment and prevention molecular studies have identified a substantial number of wssv envelope and cell surface proteins involved in the first stage of virus infection established a large number of host transcription factors replication of wssv and established some of the mechanisms by which the virus maintains a favorable host cell environment including through arresting the cell cycle changing metabolism and preventing apoptosis major gaps however remain in our understanding of how the virus upon entering the host cell via endosomes subsequently delivers its genome to the host nucleus the virion assembly process and in fact the function of the majority of wssv proteins\n",
      "\n",
      "since the virus was detected in samples collected from buddha during 8 years the presence of integrated or episomal viral genomes could be postulated besides the disruption of an oncolytic gene caused by viral integration could have been at the origin of the hs and therefore the presence of integrated viral genomes in all tissues including whole blood was investigated by means of three different methods since the integration location could not be predicted figure 1  no evidence for integrated viruses could be found in addition there was also no evidence of a circular covalently closed genomic form which could have persisted in tissues as an episomal form\n",
      "\n",
      "viruses have evolved a wide range of strategies to persist in their hosts it remains a challenge to understand the mechanisms whereby viral persistence is established and maintained especially viral persistence within a cell or group of cells mechanisms by which rna virus persistence is initiated and maintained usually involve two virusspecific factors the generation of defective interfering di particles or temperaturesensitive mutation of wildtype virus 1 2  research suggests that host factors involved in the control of persistent infection relate to elements of innate immunity in morbillivirus 3 and cellular protein synthesis in reovirus 4 \n",
      "\n",
      "when designed according to strengthening the reporting of observational studies in epidemiology criteria 23 integrated surveillance requires that both diseaseindicator outcome variables and behavioural risk factors be measured in behavioural surveys behavioural risk factors ie exposure variables simply represent the population prevalence of behaviours that may or may not increase the risk of disease without the outcome variables the exposure variables are of little use in elucidating the mechanisms of the spillover of zoonotic disease to humans or of subsequent humantohuman transmission effective surveillance requires questions that assess a range of animal exposures document experiences of unusual illness and measure contextual factors that can lead to an increase or decrease in the probabilities of behavioural risk factors and disease\n",
      "\n",
      "minion has been extensively used in detecting known 177 178 179 180 181 and new 182 pathogen strains for instance of ebola virus 183  begomovirus 184 and papillomavirus 185  curiously minion allows samples to be detected and genotyped within mere hours from sample collection even in the field sometimes literally 186 187  it has also been successfully used for rapid investigation of dengue virus outbreak in angola 188 and ebola epidemic in guinea 189  another experiment 190 allowed for deciphering full genomes of three avipoxvirus strains in realtime skipping extraction and enrichment altogether another analogous example is everglades virus evev which has been detected and subgenotyped onsite 191 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 texts\n",
    "for i in range(10):\n",
    "    print(scoreSortedCorpus['text'].iloc[i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
