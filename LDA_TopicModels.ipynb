{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Allows me to speed up tagging without using pos_tag_sents\n",
    "# as per https://stackoverflow.com/questions/33676526/pos-tagger-is-incredibly-slow\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import nltk\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "NUM_CORES = 1\n",
    "\n",
    "class LDAModelBuilder:\n",
    "    _stemLemmaTool = None\n",
    "    _stemDictionary = {}\n",
    "    _tagger = PerceptronTagger()\n",
    "\n",
    "    def __init__(self, numTopics, vectorModel, alpha, useToken, usePOS, useStemLemma, stopwordsFile, outputFile, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self._numTopics = numTopics\n",
    "        self._vectorModel = vectorModel\n",
    "        self._alpha = alpha\n",
    "        self._useToken = useToken\n",
    "        self._usePOS = usePOS\n",
    "        self._useStemLemma = useStemLemma\n",
    "        self.__initStopWords(stopwordsFile)\n",
    "        self._outputFile = outputFile\n",
    "\n",
    "    def __initStopWords(self, stopwordsFile):\n",
    "        if stopwordsFile == None:\n",
    "            if self.verbose:\n",
    "                print(\"\\tSelected 'none' for stopwords.\")\n",
    "            self._stopwords = set()\n",
    "        elif stopwordsFile == 'nltk':\n",
    "            if self.verbose:\n",
    "                print(\"\\tSelected NLTK stopwords.\")\n",
    "            self._stopwords = set(nltk_stopwords.words('english'))\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"\\tReading custom stopwords from file.\")\n",
    "            self._stopwords = set(line.strip().lower()\n",
    "                                  for line in open(stopwordsFile))\n",
    "\n",
    "    def getStopwordSet(self):\n",
    "        return self._stopwords\n",
    "\n",
    "    def __stemOrLemmatizeDocument(self, document):\n",
    "        # Stem or lemmatize document given program config\n",
    "        if self._useStemLemma == 'N':\n",
    "            return document\n",
    "        elif self._useStemLemma == 'B':\n",
    "            return self.__stemDocument(document)\n",
    "        elif self._useStemLemma == 'L':\n",
    "            return self.__lemmatizeDoc(document)\n",
    "        else:\n",
    "            print(\"Unsupported stem/lemmatization setting given in config file.\")\n",
    "\n",
    "    def __stemDocument(self, documentTokens):\n",
    "        # Stem documents using PorterStemmer\n",
    "        toStem = []\n",
    "        # process document to remove parts of speech as specified, since lemmatization function will do this automatically\n",
    "        partsSpeech = self._tagger.tag(documentTokens)\n",
    "        for word, tag in partsSpeech:\n",
    "            wntag = self.__getWordnetTag(tag)\n",
    "            if self.__keepPartOfSpeech(wntag):\n",
    "                toStem.append(word)\n",
    "\n",
    "        # Pass items through stemmer, memoizing / referencing dictionary for performance\n",
    "        toReturn = []\n",
    "        if self._stemLemmaTool is None:\n",
    "            self._stemLemmaTool = PorterStemmer()\n",
    "        for word in toStem:\n",
    "            if word not in self._stemDictionary:\n",
    "                self._stemDictionary[word] = self._stemLemmaTool.stem(word)\n",
    "            toReturn.append(self._stemDictionary[word])\n",
    "\n",
    "        return toReturn\n",
    "\n",
    "    def __lemmatizeDoc(self, documentTokens):\n",
    "        # Lemmatize the document tokens using NLTK pos_tag\n",
    "        toReturn = []\n",
    "        if self._stemLemmaTool is None:\n",
    "            self._stemLemmaTool = WordNetLemmatizer()\n",
    "        partsSpeech = self._tagger.tag(documentTokens)\n",
    "\n",
    "        for word, tag in partsSpeech:\n",
    "            wntag = self.__getWordnetTag(tag)\n",
    "            lemma = None\n",
    "            if self.__keepPartOfSpeech(wntag):\n",
    "                if wntag is None:\n",
    "                    lemma = self._stemLemmaTool.lemmatize(word)\n",
    "                else:\n",
    "                    lemma = self._stemLemmaTool.lemmatize(word, pos=wntag)\n",
    "                toReturn.append(lemma)\n",
    "        return toReturn\n",
    "\n",
    "    def __keepPartOfSpeech(self, pos):\n",
    "        # Determine if the word should be kept given its part of speech and program config\n",
    "        if self._usePOS == 'A':\n",
    "            return True\n",
    "        elif self._usePOS == 'F':\n",
    "            return (pos == wordnet.NOUN or pos == wordnet.VERB or pos == wordnet.ADJ or pos == wordnet.ADV)\n",
    "        elif self._usePOS == 'N':\n",
    "            return (pos == wordnet.NOUN or pos == wordnet.ADJ)\n",
    "        elif self._usePOS == 'n':\n",
    "            return (pos == wordnet.NOUN)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __getWordnetTag(self, tag):\n",
    "        # Convert to WordNet tags (from Penn)\n",
    "        # Source for this method: https://stackoverflow.com/a/15590384\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __isStopWord(self, word):\n",
    "        if word.lower() in self._stopwords:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __processWordToKeep(self, word):\n",
    "        if self._useToken == 'A':\n",
    "            # Keep all words except single character non-alphanumeric characters\n",
    "            if len(word) == 1 and re.search(r'\\W', word):\n",
    "                # Case for single-character nonalphanumeric\n",
    "                return None\n",
    "            else:\n",
    "                return word\n",
    "\n",
    "        elif self._useToken == 'a':\n",
    "            # Keep all words except single character non-alphanumeric characters,\n",
    "            # remove symbols if token is a mixture of alphanumeric and symbols\n",
    "            if len(word) == 1 and re.search(r'\\W', word):\n",
    "                # Case for single-character nonalphanumeric\n",
    "                return None\n",
    "            else:\n",
    "                return re.sub(r'\\W', '', word)\n",
    "\n",
    "        elif self._useToken == 'N':\n",
    "            # Keep only alphanumeric tokens\n",
    "            if re.search(r'\\W', word):\n",
    "                # Case for non-alphanumeric\n",
    "                return None\n",
    "            else:\n",
    "                # Valid case\n",
    "                return word\n",
    "\n",
    "        elif self._useToken == 'n':\n",
    "            # Keep only alphanumeric tokens, removing tokens that are only numbers\n",
    "            if re.search(r'\\W', word):\n",
    "                # Case for non-alphanumeric\n",
    "                return None\n",
    "            if not re.search(r'[a-zA-Z]', word):\n",
    "                # Case for only numbers\n",
    "                return None\n",
    "            else:\n",
    "                # Valid case\n",
    "                return word\n",
    "\n",
    "    def preProcessDocument(self, doc):\n",
    "        return self.__preProcessDocument(word_tokenize(doc))\n",
    "\n",
    "    def getBagOfWords(self, tokens):\n",
    "        return self.id2word_.doc2bow(tokens)\n",
    "\n",
    "    def __preProcessDocument(self, tokens):\n",
    "        # Perform stemming or lemmatization\n",
    "        firstPass = []\n",
    "        for word in tokens:\n",
    "            if len(word) < 3:\n",
    "                continue\n",
    "            word=word.lower()\n",
    "            keepWord = self.__processWordToKeep(word)\n",
    "            if keepWord is not None and not self.__isStopWord(keepWord):\n",
    "                firstPass.append(keepWord)\n",
    "        firstPass = self.__stemOrLemmatizeDocument(firstPass)\n",
    "        # Now we remove stop words again\n",
    "        toReturn = []\n",
    "        for word in firstPass:\n",
    "            if not self.__isStopWord(word):\n",
    "                toReturn.append(word)\n",
    "\n",
    "        return toReturn\n",
    "\n",
    "    def __buildGensimCorpus(self, documents):\n",
    "        if self.verbose:\n",
    "            print(\"\\tBuilding GenSim corpus...\")\n",
    "        # Build a GenSim corpus given documents\n",
    "        processedDocuments = []\n",
    "        for doc in documents:\n",
    "            processedDocuments.append(self.preProcessDocument(doc))\n",
    "\n",
    "        wordIDs = corpora.Dictionary(processedDocuments)\n",
    "        corpus = [wordIDs.doc2bow(text) for text in processedDocuments]\n",
    "\n",
    "        if self._vectorModel == 'B':\n",
    "            # Use binary model\n",
    "            for document in corpus:\n",
    "                document[:] = [(id, 1 if freq > 0 else 0)\n",
    "                               for (id, freq) in document]\n",
    "\n",
    "        elif self._vectorModel == 'T':\n",
    "            # Use TFIDF model\n",
    "            tfidf = gensim.models.TfidfModel(corpus)\n",
    "            corpus = tfidf[corpus]\n",
    "\n",
    "        # Else 't' use TF model (no adjustment)\n",
    "        elif not self._vectorModel == 't':\n",
    "            print(\"Unsupported vector model passed in config file.\")\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\tBuilt GenSim corpus.\")\n",
    "        return (corpus, wordIDs)\n",
    "\n",
    "    def __buildLDAModel(self, corpus, id2word):\n",
    "        return gensim.models.ldamulticore.LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            num_topics=self._numTopics,\n",
    "            alpha=self._alpha,\n",
    "            workers=NUM_CORES\n",
    "        )\n",
    "\n",
    "    def trainLDA(self, documents):\n",
    "        trainSuccess = False\n",
    "        if self.verbose:\n",
    "            print(\"\\tBuilding GenSim LDA topic model...\")\n",
    "        # Build / train LDA model via GenSim\n",
    "        corpus, wordIDs = self.__buildGensimCorpus(documents)\n",
    "        if len(wordIDs) > 0:\n",
    "            self.LDAmodel_ = self.__buildLDAModel(corpus, wordIDs)\n",
    "            self.id2word_ = wordIDs\n",
    "            self._corpus_ = corpus\n",
    "            trainSuccess = True\n",
    "            if self.verbose:\n",
    "                print(\"\\tBuilt GenSim LDA topic model.\")\n",
    "        return trainSuccess\n",
    "    \n",
    "    def saveModel_(self):\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaving LDA topic model...\")\n",
    "        # Output the model, note I use SKLearn convention with pre/post-underscore to denote pre/post train functions\n",
    "        self.LDAmodel_.save(self._outputFile + '.model')\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaved LDA topic model.\")\n",
    "\n",
    "    def loadModel(self, fromFile):\n",
    "        self.LDAmodel_ = gensim.models.ldamulticore.LdaMulticore.load(fromFile)\n",
    "            \n",
    "    def getTopic(self, topicID, n=10):\n",
    "        topic = self.LDAmodel_.get_topic_terms(topicid=topicID, topn=n)\n",
    "        # Transform word IDs back to the original word\n",
    "        topic[:] = [(self.id2word_[id], prob) for (id, prob) in topic]\n",
    "        return topic\n",
    "\n",
    "    def saveTopics_(self, n=10):\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaving LDA topics...\")\n",
    "        for topicID in range(0, self._numTopics):\n",
    "            topic = self.getTopic(topicID=topicID, n=n)\n",
    "            # Write topic output\n",
    "            with open(self._outputFile + '_' + str(topicID) + '.topic', 'w') as writer:\n",
    "                for (word, prob) in topic:\n",
    "                    writer.write(word)\n",
    "                    writer.write(' ')\n",
    "                    writer.write(str(prob))\n",
    "                    writer.write('\\n')\n",
    "        if self.verbose:\n",
    "            print(\"\\tSaved LDA topics.\")\n",
    "\n",
    "    def generateAndSaveDocTopics_(self, fileNames):\n",
    "        if self.verbose:\n",
    "            print(\"\\tGenerating and saving document topics...\")\n",
    "        # Pass file names in as vector corresponding to original corpus documents\n",
    "        with open(self._outputFile + '.dt', 'w') as writer:\n",
    "            # Iterate over documents / filenames simultaneously\n",
    "            for document, fileName in zip(self._corpus_, fileNames):\n",
    "                writer.write(fileName)\n",
    "                writer.write(' ')\n",
    "                # Get topics for document\n",
    "                docTopics = self.LDAmodel_.get_document_topics(\n",
    "                    document, minimum_probability=0)\n",
    "                # Write document topics to file\n",
    "                for (topicID, prob) in docTopics:\n",
    "                    writer.write(str(prob))\n",
    "                    writer.write(' ')\n",
    "                writer.write('\\n')\n",
    "        print(\"\\tGenerated and saved document topics.\")\n",
    "\n",
    "\n",
    "def getJaccard(s, t):\n",
    "    # Given two sets of words\n",
    "    # Calculate the Jaccard coefficient | S ⋂ T | / | S ⋃ T |\n",
    "    numer = len(s.intersection(t))\n",
    "    denom = len(s.union(t))\n",
    "    return numer / denom if denom > 0 else 0\n",
    "\n",
    "\n",
    "def getTopicSim(t1, t2):\n",
    "    # Given two topics, get their similarity as Jaccard of T1(k) and T2(k)\n",
    "    # Format note: t1, t2 should be sets of tuple representing (topic, topic_prob)\n",
    "    t1_words = set()\n",
    "    t2_words = set()\n",
    "    for ((t1_word, _), (t2_word, _)) in zip(t1, t2):\n",
    "        t1_words.add(t1_word)\n",
    "        t2_words.add(t2_word)\n",
    "    sim = getJaccard(t1_words, t2_words)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def getTopicSetSim(tprime, uprime):\n",
    "    selectedUvals = set()  # Used to see if we get a perfect match\n",
    "    simSum = 0\n",
    "    for t in tprime:\n",
    "        bestTopic = None\n",
    "        bestSim = None\n",
    "        counter = 0\n",
    "        bestTopicIndex = 0\n",
    "        for u in uprime:\n",
    "            if bestTopic is None:\n",
    "                # First index: assign base best topic, sim, index\n",
    "                bestTopic = u\n",
    "                bestTopicIndex = 0\n",
    "                bestSim = getTopicSim(t, u)\n",
    "                continue\n",
    "            # Get similarity for current topic\n",
    "            sim = getTopicSim(t, u)\n",
    "            if sim > bestSim:\n",
    "                bestSim = sim\n",
    "                bestTopic = u\n",
    "                bestTopicIndex = counter\n",
    "            counter += 1\n",
    "        selectedUvals.add(bestTopicIndex)  # Selected U at index (counter)\n",
    "        simSum += bestSim\n",
    "    if len(selectedUvals) == len(tprime):\n",
    "        # Perfect match was found\n",
    "        return (None, simSum)\n",
    "    else:\n",
    "        # Did not find a perfect match\n",
    "        return (\n",
    "            # First term: number of selected topics / number of topics in T\n",
    "            len(selectedUvals) / len(tprime),\n",
    "            simSum\n",
    "        )\n",
    "\n",
    "\n",
    "def getWordnetTag(tag):\n",
    "    # Convert to WordNet tags (from Penn)\n",
    "    # Source for this method: https://stackoverflow.com/a/15590384\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord19Path = './../scratch/CORD19Data/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xqhn0vbp</td>\n",
       "      <td>1e1286db212100993d03cc22374b624f7caee956</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Airborne rhinovirus detection and effect of ul...</td>\n",
       "      <td>10.1186/1471-2458-3-5</td>\n",
       "      <td>PMC140314</td>\n",
       "      <td>12525263.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: Rhinovirus, the most common cause ...</td>\n",
       "      <td>2003-01-13</td>\n",
       "      <td>Myatt, Theodore A; Johnston, Sebastian L; Rudn...</td>\n",
       "      <td>BMC Public Health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gi6uaa83</td>\n",
       "      <td>8ae137c8da1607b3a8e4c946c07ca8bda67f88ac</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Discovering human history from stomach bacteria</td>\n",
       "      <td>10.1186/gb-2003-4-5-213</td>\n",
       "      <td>PMC156578</td>\n",
       "      <td>12734001.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Recent analyses of human pathogens have reveal...</td>\n",
       "      <td>2003-04-28</td>\n",
       "      <td>Disotell, Todd R</td>\n",
       "      <td>Genome Biol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>le0ogx1s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC</td>\n",
       "      <td>A new recruit for the army of the men of death</td>\n",
       "      <td>10.1186/gb-2003-4-7-113</td>\n",
       "      <td>PMC193621</td>\n",
       "      <td>12844350.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>The army of the men of death, in John Bunyan's...</td>\n",
       "      <td>2003-06-27</td>\n",
       "      <td>Petsko, Gregory A</td>\n",
       "      <td>Genome Biol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fy4w7xz8</td>\n",
       "      <td>0104f6ceccf92ae8567a0102f89cbb976969a774</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Association of HLA class I with severe acute r...</td>\n",
       "      <td>10.1186/1471-2350-4-9</td>\n",
       "      <td>PMC212558</td>\n",
       "      <td>12969506.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: The human leukocyte antigen (HLA) ...</td>\n",
       "      <td>2003-09-12</td>\n",
       "      <td>Lin, Marie; Tseng, Hsiang-Kuang; Trejaut, Jean...</td>\n",
       "      <td>BMC Med Genet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0qaoam29</td>\n",
       "      <td>5b68a553a7cbbea13472721cd1ad617d42b40c26</td>\n",
       "      <td>PMC</td>\n",
       "      <td>A double epidemic model for the SARS propagation</td>\n",
       "      <td>10.1186/1471-2334-3-19</td>\n",
       "      <td>PMC222908</td>\n",
       "      <td>12964944.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: An epidemic of a Severe Acute Resp...</td>\n",
       "      <td>2003-09-10</td>\n",
       "      <td>Ng, Tuen Wai; Turinici, Gabriel; Danchin, Antoine</td>\n",
       "      <td>BMC Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  xqhn0vbp  1e1286db212100993d03cc22374b624f7caee956      PMC   \n",
       "1  gi6uaa83  8ae137c8da1607b3a8e4c946c07ca8bda67f88ac      PMC   \n",
       "2  le0ogx1s                                       NaN      PMC   \n",
       "3  fy4w7xz8  0104f6ceccf92ae8567a0102f89cbb976969a774      PMC   \n",
       "4  0qaoam29  5b68a553a7cbbea13472721cd1ad617d42b40c26      PMC   \n",
       "\n",
       "                                               title                      doi  \\\n",
       "0  Airborne rhinovirus detection and effect of ul...    10.1186/1471-2458-3-5   \n",
       "1    Discovering human history from stomach bacteria  10.1186/gb-2003-4-5-213   \n",
       "2     A new recruit for the army of the men of death  10.1186/gb-2003-4-7-113   \n",
       "3  Association of HLA class I with severe acute r...    10.1186/1471-2350-4-9   \n",
       "4   A double epidemic model for the SARS propagation   10.1186/1471-2334-3-19   \n",
       "\n",
       "       pmcid   pubmed_id license  \\\n",
       "0  PMC140314  12525263.0   no-cc   \n",
       "1  PMC156578  12734001.0   no-cc   \n",
       "2  PMC193621  12844350.0   no-cc   \n",
       "3  PMC212558  12969506.0   no-cc   \n",
       "4  PMC222908  12964944.0   no-cc   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  BACKGROUND: Rhinovirus, the most common cause ...   2003-01-13   \n",
       "1  Recent analyses of human pathogens have reveal...   2003-04-28   \n",
       "2  The army of the men of death, in John Bunyan's...   2003-06-27   \n",
       "3  BACKGROUND: The human leukocyte antigen (HLA) ...   2003-09-12   \n",
       "4  BACKGROUND: An epidemic of a Severe Acute Resp...   2003-09-10   \n",
       "\n",
       "                                             authors            journal  \\\n",
       "0  Myatt, Theodore A; Johnston, Sebastian L; Rudn...  BMC Public Health   \n",
       "1                                   Disotell, Todd R        Genome Biol   \n",
       "2                                  Petsko, Gregory A        Genome Biol   \n",
       "3  Lin, Marie; Tseng, Hsiang-Kuang; Trejaut, Jean...      BMC Med Genet   \n",
       "4  Ng, Tuen Wai; Turinici, Gabriel; Danchin, Antoine     BMC Infect Dis   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN           True   \n",
       "1                          NaN            NaN           True   \n",
       "2                          NaN            NaN          False   \n",
       "3                          NaN            NaN           True   \n",
       "4                          NaN            NaN           True   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0               True  custom_license   \n",
       "1               True  custom_license   \n",
       "2               True  custom_license   \n",
       "3               True  custom_license   \n",
       "4               True  custom_license   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...  \n",
       "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_path = cord19Path + 'metadata.csv'\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-88e3a6145d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#read biomed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiorxiv_medrxiv_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJsonToDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mbiomed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbiomed_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-88e3a6145d76>\u001b[0m in \u001b[0;36mJsonToDataFrame\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;34m'section'\u001b[0m  \u001b[0;34m:\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;34m'text'\u001b[0m     \u001b[0;34m:\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             }, ignore_index=True)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7112\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7113\u001b[0m                 \u001b[0mcombined_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7114\u001b[0;31m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7115\u001b[0m             other = DataFrame(\n\u001b[1;32m   7116\u001b[0m                 \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, index, **kwargs)\u001b[0m\n\u001b[1;32m   4219\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4223\u001b[0m     def drop(\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4496\u001b[0m         if all(\n\u001b[1;32m   4497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4498\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4499\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4500\u001b[0m         ):\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4498\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4499\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4500\u001b[0m         ):\n\u001b[1;32m   4501\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36midentical\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4414\u001b[0m         \"\"\"\n\u001b[1;32m   4415\u001b[0m         return (\n\u001b[0;32m-> 4416\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4417\u001b[0m             and all(\n\u001b[1;32m   4418\u001b[0m                 (\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mequals\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4391\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4393\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4394\u001b[0m             \u001b[0;31m# if other is not object, use other's logic for coercion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4395\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_object_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_is_dtype_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "    @Desc    : Reads in json article and converts into Pandas Dataframe\n",
    "    @Params  : filepath (str)\n",
    "    @Returns : Pandas Dataframe \n",
    "'''\n",
    "def JsonToDataFrame(filepath):\n",
    "        \n",
    "    #read json into dict\n",
    "    with open(filepath) as json_data:\n",
    "        data = json.load(json_data)\n",
    "        \n",
    "        paper_id = data['paper_id']\n",
    "        abstract = '\\n'.join([section['text'] for section in data['abstract']])\n",
    "\n",
    "        \n",
    "\n",
    "        final_data = {\n",
    "            'paper_id'  : [data['paper_id']],\n",
    "            'section'   : ['abstract'],\n",
    "            'text'  : ['\\n'.join([section['text'] for section in data['abstract']])]                                       \n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(final_data)\n",
    "        for section in data['body_text']:\n",
    "            df = df.append({\n",
    "                'paper_id' : data['paper_id'],\n",
    "                'section'  : section['section'],\n",
    "                'text'     : section['text']\n",
    "            }, ignore_index=True)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "        \n",
    "biorxiv_medrxiv    = cord19Path + 'biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/'\n",
    "comm_use_subset    = cord19Path + 'comm_use_subset/comm_use_subset/pdf_json/'\n",
    "noncomm_use_subset = cord19Path + 'noncomm_use_subset/noncomm_use_subset/pdf_json/'\n",
    "\n",
    "biorxiv_medrxiv_files       = [biorxiv_medrxiv + pos_json for pos_json in os.listdir(biorxiv_medrxiv) if pos_json.endswith('.json')]\n",
    "comm_use_subset_files       = [comm_use_subset + pos_json for pos_json in os.listdir(comm_use_subset) if pos_json.endswith('.json')]\n",
    "noncomm_use_subset_files    = [noncomm_use_subset + pos_json for pos_json in os.listdir(noncomm_use_subset) if pos_json.endswith('.json')]\n",
    "\n",
    "#initialize dfs\n",
    "biomed_df      = pd.DataFrame()\n",
    "comm_use_df    = pd.DataFrame()\n",
    "noncomm_use_df = pd.DataFrame()\n",
    "\n",
    "#read biomed data\n",
    "for f in biorxiv_medrxiv_files:\n",
    "    df = JsonToDataFrame(f)\n",
    "    biomed_df = biomed_df.append(df, ignore_index=True)\n",
    "\n",
    "#read commonly used data\n",
    "for f in comm_use_subset_files:\n",
    "    df = JsonToDataFrame(f)\n",
    "    comm_use_df = comm_use_df.append(df, ignore_index=True)\n",
    "\n",
    "#read non-commonly used data\n",
    "for f in noncomm_use_subset_files:\n",
    "    df = JsonToDataFrame(f)\n",
    "    noncomm_use_df = noncomm_use_df.append(df, ignore_index=True)\n",
    "\n",
    "\n",
    "full_corpus = pd.concat([biomed_df, comm_use_df, noncomm_use_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus['section'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "#remove punctuation\n",
    "full_corpus['text'] = full_corpus['text'].map(lambda x: x.translate(punct_table))\n",
    "\n",
    "#convert to lowercase\n",
    "full_corpus['text'] = full_corpus['text'].map(lambda x: x.lower())\n",
    "\n",
    "full_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_corpus)  # number of documents is almost 500k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('virus', 0.026144851), ('livestock', 0.024827562), ('evidence', 0.024739252), ('origin', 0.024711605), ('host', 0.023042759), ('geographic', 0.02176513), ('surveillance', 0.021404535), ('farmer', 0.01911933), ('risk', 0.01848709), ('circulation', 0.017481582), ('strain', 0.016416615), ('genome', 0.016375087), ('farm', 0.016251558), ('variation', 0.016000096), ('southeast', 0.015954487)]\n",
      "[('virus', 0.03267227), ('evidence', 0.03025709), ('farmer', 0.025195641), ('host', 0.024447901), ('risk', 0.02253021), ('surveillance', 0.020325229), ('livestock', 0.019785993), ('origin', 0.019198526), ('geographic', 0.017966416), ('know', 0.016745778), ('whole', 0.016386623), ('literature', 0.016286649), ('genetics', 0.016285105), ('difference', 0.015902014), ('animal', 0.015827892)]\n",
      "[('virus', 0.034556378), ('evidence', 0.030068308), ('livestock', 0.025020516), ('risk', 0.02423982), ('surveillance', 0.023939574), ('host', 0.022375813), ('geographic', 0.022373198), ('origin', 0.021125786), ('farmer', 0.0202609), ('report', 0.016940922), ('temporal', 0.016262125), ('genetic', 0.016099889), ('agreement', 0.01600688), ('information', 0.01593636), ('evolution', 0.015325401)]\n",
      "[('virus', 0.031556338), ('evidence', 0.026364904), ('risk', 0.023804609), ('surveillance', 0.02377906), ('livestock', 0.023526957), ('geographic', 0.021911955), ('origin', 0.020837083), ('farmer', 0.018333718), ('host', 0.01701546), ('time', 0.016526796), ('serve', 0.01621703), ('inform', 0.016178813), ('range', 0.015977312), ('mechanism', 0.015747359), ('genetic', 0.015735209)]\n",
      "[('virus', 0.030767236), ('evidence', 0.030102912), ('host', 0.025073953), ('farmer', 0.023547279), ('surveillance', 0.023431035), ('geographic', 0.021162411), ('origin', 0.02028904), ('risk', 0.019980334), ('livestock', 0.019679302), ('southeast', 0.016805764), ('diagnostics', 0.016175954), ('distribution', 0.016123505), ('genomic', 0.01599118), ('sustainable', 0.015958784), ('behavioral', 0.015952528)]\n",
      "[('evidence', 0.024899857), ('farmer', 0.023688903), ('surveillance', 0.02322484), ('geographic', 0.023185609), ('virus', 0.022386244), ('origin', 0.022261215), ('livestock', 0.020819591), ('risk', 0.020665385), ('host', 0.017677125), ('mixed', 0.017027035), ('time', 0.017012157), ('evolution', 0.016718466), ('difference', 0.016456403), ('set', 0.016453383), ('management', 0.016409086)]\n",
      "[('evidence', 0.029942978), ('virus', 0.027532982), ('risk', 0.025017478), ('farmer', 0.023185149), ('livestock', 0.022463636), ('host', 0.022345822), ('geographic', 0.021047933), ('origin', 0.018703075), ('surveillance', 0.01866442), ('socioeconomic', 0.018087955), ('reduction', 0.0166132), ('epidemic', 0.016462032), ('strain', 0.016325388), ('agreement', 0.01629715), ('human', 0.015586417)]\n",
      "[('evidence', 0.039269917), ('virus', 0.028974324), ('origin', 0.022252202), ('risk', 0.021968232), ('livestock', 0.02169681), ('host', 0.021097416), ('surveillance', 0.019459825), ('farmer', 0.019403279), ('geographic', 0.018726818), ('asia', 0.017330462), ('epidemic', 0.016530983), ('receptor', 0.016367462), ('therapeutic', 0.016308285), ('difference', 0.015952688), ('sustainable', 0.01573402)]\n",
      "[('surveillance', 0.024841392), ('virus', 0.023476189), ('origin', 0.022940755), ('geographic', 0.022570524), ('evidence', 0.022393594), ('host', 0.022263944), ('risk', 0.020898871), ('farmer', 0.020330666), ('livestock', 0.018246122), ('dissemination', 0.016872767), ('strategy', 0.016816469), ('mixed', 0.01641273), ('strain', 0.016276196), ('serve', 0.016170138), ('literature', 0.016146362)]\n",
      "[('virus', 0.035931826), ('evidence', 0.028182814), ('origin', 0.025917094), ('farmer', 0.02319882), ('risk', 0.021564247), ('host', 0.021551637), ('livestock', 0.021064466), ('geographic', 0.019230723), ('surveillance', 0.01922622), ('sequencing', 0.017834961), ('factor', 0.017597968), ('therapeutic', 0.016131451), ('access', 0.016108235), ('reservoir', 0.015883544), ('measure', 0.015765078)]\n",
      "[('evidence', 0.029345328), ('virus', 0.028925207), ('surveillance', 0.024081022), ('livestock', 0.023737248), ('farmer', 0.02348372), ('host', 0.023196282), ('geographic', 0.022921387), ('risk', 0.022198081), ('origin', 0.02159652), ('genome', 0.01766246), ('field', 0.016317971), ('factor', 0.015939664), ('receptor', 0.015768567), ('circulation', 0.015753692), ('development', 0.015529519)]\n",
      "[('virus', 0.02844343), ('evidence', 0.026383251), ('geographic', 0.024048764), ('origin', 0.023930479), ('surveillance', 0.02322097), ('livestock', 0.02191982), ('farmer', 0.021634942), ('host', 0.020648025), ('risk', 0.01982133), ('protocol', 0.01772595), ('behavioral', 0.016498534), ('rapid', 0.016412888), ('inform', 0.016146246), ('distribution', 0.015891632), ('measure', 0.015822118)]\n",
      "[('evidence', 0.032429148), ('virus', 0.028671008), ('host', 0.024512952), ('farmer', 0.022245424), ('livestock', 0.021458525), ('geographic', 0.020296847), ('risk', 0.019863706), ('surveillance', 0.019595593), ('origin', 0.018858228), ('temporal', 0.018246084), ('diverse', 0.01662797), ('report', 0.015829789), ('sustainable', 0.015751261), ('management', 0.015662901), ('asia', 0.015607182)]\n",
      "[('evidence', 0.02681577), ('virus', 0.026309524), ('geographic', 0.023283305), ('risk', 0.022899454), ('origin', 0.021988917), ('host', 0.019739285), ('livestock', 0.01932796), ('farmer', 0.018178446), ('rapid', 0.0176323), ('serve', 0.017276488), ('surveillance', 0.017192751), ('protocol', 0.016678942), ('genetics', 0.016666645), ('distribution', 0.016319921), ('genomic', 0.01615592)]\n",
      "[('evidence', 0.030079849), ('virus', 0.024808358), ('geographic', 0.023367193), ('farmer', 0.02185415), ('surveillance', 0.021342147), ('livestock', 0.02011065), ('risk', 0.019672437), ('origin', 0.019160392), ('host', 0.01865973), ('infection', 0.01755199), ('experimental', 0.016922109), ('diagnostics', 0.016302109), ('agreement', 0.01619071), ('rapid', 0.01613413), ('sequencing', 0.015987054)]\n"
     ]
    }
   ],
   "source": [
    "# Seek to find groups of topics that match this prompt\n",
    "\n",
    "promptText = '''\n",
    "What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface?\n",
    "Specifically, we want to know what the literature reports about:\n",
    "Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\n",
    "Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\n",
    "Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.\n",
    "Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\n",
    "Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\n",
    "Experimental infections to test host range for this pathogen.\n",
    "Animal host(s) and any evidence of continued spill-over to humans\n",
    "Socioeconomic and behavioral risk factors for this spill-over\n",
    "Sustainable risk reduction strategies\n",
    "'''\n",
    "\n",
    "promptDocs = [promptText]\n",
    "\n",
    "baseModelOutputPath = '../scratch/CORD19_Topic_Models/'\n",
    "\n",
    "ldaParams = dict(\n",
    "    numTopics = 15,           # 15 topics\n",
    "    vectorModel = 't',        # term freq detection model\n",
    "    alpha = 5,                # higher alpha for sharp topic detection\n",
    "    useToken = 'n',           # strictest token filtering \n",
    "    usePOS = 'N',             # nouns or adv \n",
    "    useStemLemma = 'L',       # lemmatization\n",
    "    stopwordsFile = 'nltk',   # LDA stopwords\n",
    "    outputFile = baseModelOutputPath + 'prompt_base_model',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "topicsToMatch = LDAModelBuilder(\n",
    "    **ldaParams\n",
    ")\n",
    "\n",
    "topicsToMatch.trainLDA(promptDocs)\n",
    "topicsToMatch.saveTopics_()\n",
    "\n",
    "k = 15 # number of words per topic\n",
    "promptTopicSet = []\n",
    "for i in range(0, ldaParams['numTopics']):\n",
    "    topic = topicsToMatch.getTopic(topicID=i, n=k)\n",
    "    print(topic)\n",
    "    promptTopicSet.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Now get topics for each document\n",
    "ldaModels = dict()\n",
    "\n",
    "numEmpty=0\n",
    "\n",
    "minDocLength = 10\n",
    "\n",
    "for i in range(len(full_corpus)):\n",
    "    docText = [full_corpus['text'].iloc[i]]\n",
    "    ldaParams['outputFile'] = baseModelOutputPath + '_model_doc_' + str(i)\n",
    "    ldaModels[i] = LDAModelBuilder(**ldaParams)\n",
    "    if ldaModels[i].trainLDA(docText):\n",
    "        # Save if train successful\n",
    "        ldaModels[i].saveTopics_()\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        \n",
    "print(\"Processed\", i, \"documents.\")\n",
    "print(\"Found\", numEmpty,\"empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multiprocessing for above code\n",
    "from multiprocessing import Pool, Lock, Process\n",
    "from multiprocessing.sharedctypes import Array\n",
    "\n",
    "\n",
    "ldaModels = dict()\n",
    "\n",
    "numEmpty=0\n",
    "\n",
    "minDocLength = 10\n",
    "\n",
    "\n",
    "def processLDA(lowerIndex, upperIndex, documents):\n",
    "    for (doc, i) in zip(documents, range(lowerIndex, upperIndex)):\n",
    "        docText = [doc]\n",
    "        ldaParams['outputFile'] = baseModelOutputPath + '_model_doc_' + str(i)\n",
    "        model = LDAModelBuilder(**ldaParams)\n",
    "        if model.trainLDA(docText):\n",
    "            model.saveTopics_()\n",
    "\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    numCores = 36\n",
    "    totalDocs = len(full_corpus)\n",
    "\n",
    "    processes = []\n",
    "    splits = np.array_split(full_corpus, numCores)\n",
    "    \n",
    "    lower = 0\n",
    "    \n",
    "    for i in range(numCores):\n",
    "        # spawn process with docs\n",
    "        docs = splits[i].text.tolist()\n",
    "        p = Process(target=processLDA, args=(lower, lower+len(docs), docs))\n",
    "        p.start()\n",
    "        print(\"Started process\", i)\n",
    "        processes.append(p)\n",
    "\n",
    "        lower += len(docs)\n",
    "\n",
    "        \n",
    "        \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        print(\"Joined process.\")\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"Processed\", i, \"documents.\")\n",
    "    print(\"Found\", numEmpty,\"empty.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alloc array\n",
      "read in  0 models.\n",
      "read in  100 models.\n",
      "read in  200 models.\n",
      "read in  300 models.\n",
      "read in  400 models.\n",
      "read in  500 models.\n",
      "read in  600 models.\n",
      "read in  700 models.\n",
      "read in  800 models.\n",
      "read in  900 models.\n",
      "read in  1000 models.\n",
      "read in  1100 models.\n",
      "read in  1200 models.\n",
      "read in  1300 models.\n",
      "read in  1400 models.\n",
      "read in  1500 models.\n",
      "read in  1600 models.\n",
      "read in  1700 models.\n",
      "read in  1800 models.\n",
      "read in  1900 models.\n",
      "read in  2000 models.\n",
      "read in  2100 models.\n",
      "read in  2200 models.\n",
      "read in  2300 models.\n",
      "read in  2400 models.\n",
      "read in  2500 models.\n",
      "read in  2600 models.\n",
      "read in  2700 models.\n",
      "read in  2800 models.\n",
      "read in  2900 models.\n",
      "read in  3000 models.\n",
      "read in  3100 models.\n",
      "read in  3200 models.\n",
      "read in  3300 models.\n",
      "read in  3400 models.\n",
      "read in  3500 models.\n",
      "read in  3600 models.\n",
      "read in  3700 models.\n",
      "read in  3800 models.\n",
      "read in  3900 models.\n",
      "read in  4000 models.\n",
      "read in  4100 models.\n",
      "read in  4200 models.\n",
      "read in  4300 models.\n",
      "read in  4400 models.\n",
      "read in  4500 models.\n",
      "read in  4600 models.\n",
      "read in  4700 models.\n",
      "read in  4800 models.\n",
      "read in  4900 models.\n",
      "read in  5000 models.\n",
      "read in  5100 models.\n",
      "read in  5200 models.\n",
      "read in  5300 models.\n",
      "read in  5400 models.\n",
      "read in  5500 models.\n",
      "read in  5600 models.\n",
      "read in  5700 models.\n",
      "read in  5800 models.\n",
      "read in  5900 models.\n",
      "read in  6000 models.\n",
      "read in  6100 models.\n",
      "read in  6200 models.\n",
      "read in  6300 models.\n",
      "read in  6400 models.\n",
      "read in  6500 models.\n",
      "read in  6600 models.\n",
      "read in  6700 models.\n",
      "read in  6800 models.\n",
      "read in  6900 models.\n",
      "read in  7000 models.\n",
      "read in  7100 models.\n",
      "read in  7200 models.\n",
      "read in  7300 models.\n",
      "read in  7400 models.\n",
      "read in  7500 models.\n",
      "read in  7600 models.\n",
      "read in  7700 models.\n",
      "read in  7800 models.\n",
      "read in  7900 models.\n",
      "read in  8000 models.\n",
      "read in  8100 models.\n",
      "read in  8200 models.\n",
      "read in  8300 models.\n",
      "read in  8400 models.\n",
      "read in  8500 models.\n",
      "read in  8600 models.\n",
      "read in  8700 models.\n",
      "read in  8800 models.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e305d2d21a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#if path.exists(readFile + '_' + str(t) + '.topic'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadFile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtopicFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtopicLine\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopicFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopicLine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[0;34m(do_setlocale)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_setlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnl_langinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCODESET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read in calculated topic models\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "import numpy\n",
    "\n",
    "ldaDocumentTopics = numpy.empty(500000, dtype=object)\n",
    "print(\"alloc array\")\n",
    "\n",
    "numFailed = 0\n",
    "\n",
    "numTopics = 15\n",
    "\n",
    "for i in range(500000):\n",
    "    readFile = baseModelOutputPath + '_model_doc_' + str(i)\n",
    "    \n",
    "    ldaDocumentTopics[i] = list()\n",
    "    \n",
    "    for t in range(numTopics):\n",
    "        topics = []\n",
    "        #if path.exists(readFile + '_' + str(t) + '.topic'):\n",
    "        try:\n",
    "            with open(readFile + '_' + str(t) + '.topic', 'r') as topicFile:\n",
    "                for topicLine in topicFile:\n",
    "                    items = topicLine.split()\n",
    "                    topicString = items[0]\n",
    "                    topicWeight = float(items[1])\n",
    "                    topics.append((topicString, topicWeight))\n",
    "        except OSError:\n",
    "            pass\n",
    "        ldaDocumentTopics[i].append(topics)      \n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"read in \", i, \"models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now find the n most similar documents\n",
    "k = 30\n",
    "\n",
    "def getTopicsForDocument(docID):\n",
    "    topics = []\n",
    "    for i in range(0, ldaParams['numTopics']):\n",
    "        topics.append(ldaModels[docID].getTopic(topicID=i, n=k))\n",
    "    return topics\n",
    "\n",
    "for i in range(0, len(full_corpus)):\n",
    "    docTopics = getTopicsForDocument(i)\n",
    "    full_corpus['aggSimScore'] = getTopicSetSim(docTopics, promptTopicSet)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by highest sim scores\n",
    "full_corpus.sort_values('aggSimScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new sim metric from\n",
    "# Wang, Xi. (2019). Evaluating Similarity Metrics for Latent Twitter Topics. \n",
    "\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./../scratch/GoogleNewsVectors.gz', binary=True)  \n",
    "\n",
    "def we_basedSimScore(topicSet1, topicSet2):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
